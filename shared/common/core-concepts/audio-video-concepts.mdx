### Basic audio and video interaction workflow

The following diagram shows the basic workflow of using the <Vg k="COMPANY" /> SDK to implement basic audio and video interaction.

![orientation_adaptive_locked_landscape](/images/common/basic-audio-and-video.svg)

<Vg k="COMPANY" /> uses the following basic concepts:

### Track

A track contains specific audio or video information and consists of three parts: input source, filter, and output end. According to the different functions in the RTC process, tracks can be further classified into uplink tracks (the sending end) and downlink tracks (the receiving end).

![Track](/images/common/track.svg)

#### Input source

Located at the beginning of the track, the input source represents the local audio and video data to be published. It can be a camera, a screen capture, a microphone, an audio or video parsed from a file, and so on.

#### Filter 

Performs a series of processing operations on audio and video, including pre-processing and post-processing, and transmits the processed audio and video signals to the output end. Filters can be connected to multiple input sources or multiple outputs.

    - Pre-processing: Audio/video filters in the sender track, such as virtual background, beautification, echo cancellation, noise reduction, and so on.
    - Post-processing : Audio/video filters in the receiving track , such as super-resolution, spatial sound effects, and so on.

#### Output

Located at the end of the track, such as an encoder, or a renderer.

### Audio module

In audio interaction, the main functions of the audio module are as shown in the figure below:

![An audio module](/images/common/audio-module.svg)

<Admonition type="info" title="Information">
 After calling `registerAudioFrameObserver`, you can obtain the raw audio data at different observation points in the audio process, as detailed below:
- ①: Obtain the raw audio data of ear monitoring through the `onEarMonitoringAudioFrame` callback.
- ②: Obtain the captured raw audio data through the `onRecordAudioFrame` callback.
- ③: Obtain the raw audio playback data of each individual stream through the `onPlaybackAudioFrameBeforeMixing` callback.
- ④: Obtain the raw audio playback data of all mixed streams through the `onPlaybackAudioFrame` callback.
- ⑤: Obtain the raw audio data after mixing the captured and playback audio through the `onMixedAudioFrame` callback.  
    i.e., ⑤ `onMixedAudioFrame` = ② `onRecordAudioFrame` + ④ `onPlaybackAudioFrame`
</Admonition>

#### Audio routing

The audio output device used by the app when playing audio. Common audio routes include wired headphones, earpieces, speakers, Bluetooth headphones, and others.

  The APIs used by the audio module are as follows:

  - Enable local audio collection: `enableLocalAudio`
  - Set local playback device: `setPlaybackDevice`
  - Set up audio routing: `setDefaultAudioRouteToSpeakerphone`

### Video module

The following diagram shows the main functions of the video module in video interaction:

![Video module functions](/images/common/video-module.svg)

<Admonition type="info" title="Information">
- ①: Observation point is `POSITION_POST_CAPTURER_ORIGIN`.
- ②: Observation point is `POSITION_POST_CAPTURER`, corresponding to the `onCaptureVideoFrame` callback.
- ③: Observation point is `POSITION_PRE_ENCODER`, corresponding to the `onPreEncodeVideoFrame` callback.
- ④: Observation point is `POSITION_PRE_RENDERER`, corresponding to the `onRenderVideoFrame` callback.
</Admonition>

The APIs used by the video module are as follows:

- Enable local video collection: `enableLocalVideo`
- Local preview: `setupLocalVideo` → `startPreview`
- Video rendering shows: `setupRemoteVideo`

<PlatformWrapper platform="unity">

### Implement a custom video source

In the following code example, you create the basic framework required to push video frames from a custom source. Depending on the type of your source, you add your own code to this framework that converts the source data to `VideoFrame` data. To create the basic framework, take the following steps:

1. **Import the required <Vg k="COMPANY" /> and Android libraries**

    You use the Android `TextureView` and `SurfaceTexture` objects for rendering custom video. The video data from the `SurfaceTexture` is converted to a `VideoFrame` before it is pushed to the channel. To use these libraries in your <Vpl k="CLIENT" />, add the following statements after the last `import` statement in `/app/java/com.example.<projectname>/MainActivity`.

    ```java
    import io.agora.base.VideoFrame;
    import android.graphics.SurfaceTexture;
    import android.view.TextureView;
    import androidx.annotation.NonNull;
    ```

1. **Define variables to process and push video data**

    In `/app/java/com.example.<projectname>/MainActivity`, add the following declarations to the `MainActivity` class:

    ```java
    private bool mTextureDestroyed = false;
    private bool mPreviewing = false;
    private const int SAMPLE_RATE = 48000;
            private RingBuffer<byte> _audioBuffer;
                        private Thread _pushAudioFrameThread;


    ```

1. **Enable publishing the custom audio track**

    When a user presses **Join**, you configure `ChannelMediaOptions` to enable publishing of the captured video from a custom source. You set the external video source, and set up a `TextureView` for the custom video preview. To do this: 

    1. Add the following lines to the `joinChannel(View view)` method in the `MainActivity` class after `ChannelMediaOptions options = new ChannelMediaOptions();`:

        ```java
        // Enable publishing of the captured video from a custom source
        options.publishCustomVideoTrack = true;
        // Configure the external video source.
        agoraEngine.setExternalVideoSource(true, true, Constants.ExternalVideoSourceType.VIDEO_FRAME);
        // Check whether texture encoding is supported
        showMessage(agoraEngine.isTextureEncodeSupported() ? "Texture encoding is supported" : 
                "Texture encoding is not supported");
                // Set up a preview TextureView for the custom video
        setupCustomLocalVideoPreview();
        // Show the preview TextureView
        previewTextureView.setVisibility(View.VISIBLE);
        ```

    1. In the `joinChannel(View view)` method, *remove* the following lines:

        ```java
        setupLocalVideo();
        localSurfaceView.setVisibility(View.VISIBLE);
        ```

1. **Set up a TextureView for the custom video**

    Create a new `TextureView` object, and add a `SurfaceTextureListener` to it. The listener triggers the `onSurfaceTextureAvailable` callback when a `SurfaceTexture` becomes available. You add the `TextureView` to the `FrameLayout` container to display it in the UI. To do this, add the following method to the `MainActivity` class:

    ```java
    private void setupCustomLocalVideoPreview() {
        // Create TextureView
        previewTextureView = new TextureView(getBaseContext());
        // Add a SurfaceTextureListener
        previewTextureView.setSurfaceTextureListener(surfaceTextureListener);
        // Add the TextureView to the local video FrameLayout
        FrameLayout container = findViewById(R.id.local_video_view_container);
        container.addView(previewTextureView,320,240);
    }
    ```

1. **Define the `SurfaceTextureListener`**

    When a `SurfaceTexture` becomes available, you create a `previewSurfaceTexture` and set its `onFrameAvailableListener` listener. You set up and configure your custom video source, set its `SurfaceTexture` to the `previewSurfaceTexture`, and start the preview. To do this, add the following definition of the `surfaceTextureListener` to the `MainActivity` class:

    ```java
    private final TextureView.SurfaceTextureListener surfaceTextureListener = new TextureView.SurfaceTextureListener(){
        @Override
        public void onSurfaceTextureAvailable(@NonNull SurfaceTexture surface, int width, int height) {
            // Invoked when a TextureView's SurfaceTexture is ready for use.

            if (mPreviewing) {
                // Already previewing custom video
                return;
            }

            showMessage("Surface Texture Available");
            mTextureDestroyed = false;

            // Set up previewSurfaceTexture
            previewSurfaceTexture = new SurfaceTexture(true);
            previewSurfaceTexture.setOnFrameAvailableListener(onFrameAvailableListener);

            // Add code here to set up and configure the custom video source 
            // Add code here to set SurfaceTexture of the custom video source to previewSurfaceTexture
            
            // Start preview
            mPreviewing = true;
        }

        @Override
        public void onSurfaceTextureSizeChanged(@NonNull SurfaceTexture surface, int width, int height) {
        }

        @Override
        public boolean onSurfaceTextureDestroyed(@NonNull SurfaceTexture surface) {
            mTextureDestroyed = true;
            return false;
        }

        @Override
        public void onSurfaceTextureUpdated(@NonNull SurfaceTexture surface) {

        }
    };
    ```

1. **Push the video frames**

    The `onFrameAvailableListener` callback is triggered when a new video frame is available. In the callback, you convert the `SurfaceTexture` data to a <Vg k="VSDK" /> `VideoFrame` and push the frame to the channel.
    To do this, add the following `OnFrameAvailableListener` to the `MainActivity` class:

    ```java
    private final SurfaceTexture.OnFrameAvailableListener onFrameAvailableListener = new SurfaceTexture.OnFrameAvailableListener() {
        @Override
        public void onFrameAvailable(SurfaceTexture surfaceTexture) {
            // Callback to notify that a new stream video frame is available.
            
            if (isJoined) {
                // Configure the external video frames and send them to the SDK
                VideoFrame videoFrame = null;

                // Add code here to convert the surfaceTexture data to a VideoFrame object

                // Send VideoFrame to the SDK
                agoraEngine.pushExternalVideoFrame(videoFrame);
            }
        }
    };
    ```

### Implement a custom audio source

To push audio from a custom source to a channel, take the following steps:

1. **Import the required Android and Java libraries**

    You use an `InputStream` to read the contents of the custom audio source. The <Vpl k="CLIENT" /> starts a separate `Process` to read and push the audio data. To use these libraries in your <Vpl k="CLIENT" />, add the following statements after the last `import` statement in `/app/java/com.example.<projectname>/MainActivity`.

    ```java
    import android.os.Process;
    import java.io.InputStream;
    import java.io.IOException;
    ```

1. **Define variables to manage and push the audio stream**

    In `/app/java/com.example.<projectname>/MainActivity`, add the following declarations to the `MainActivity` class:

    ```java
    // Please do not change this value because Unity re-samples the sample rate to 48000.
    private const int SAMPLE_RATE = 48000;
    private const int SAMPLE_NUM_OF_CHANNEL = 2;
    private const int BYTES_PER_SAMPLE = 16;
    private const int SAMPLES = 441;
    private const int BUFFER_SIZE = SAMPLES * BYTES_PER_SAMPLE / 8 * SAMPLE_NUM_OF_CHANNEL;
    private const int PUSH_INTERVAL = SAMPLES * 1000 / SAMPLE_RATE;
    private const int PUSH_FREQ_PER_SEC = 100;
    private RingBuffer<byte> _audioBuffer;
    private bool _startConvertSignal = false;
    private Thread _pushAudioFrameThread;
    private System.Object _pushAudioFrameThreadSignal = new System.Object();
    private int _count;
    private bool _startSignal = false;
    ```

1. **Add a raw audio file to the project**

    In this example, you use an audio file as the source of your custom audio data. To add the audio file to your Android project, create a folder `app\src\main\assets` and add a sample audio file in `*.wav` or `*.raw` format to this folder. Update the value of the `AUDIO_FILE` variable to show the audio file name. Also make sure that the values of the audio file parameters in your code match the audio file you placed in the assets folder.

1. **Enable publishing the custom audio track**

    When a user presses **Join**, you set the `ChannelMediaOptions` to disable the microphone audio track and enable the custom audio track. You also enable custom audio local playback and set the external audio source. To do this, add the following lines to the `joinChannel(View view)` method in the `MainActivity` class after `options.clientRoleType = Constants.CLIENT_ROLE_BROADCASTER;`:

    ```csharp
    private void setExternalVideoSource()
    {
        options.publishMicrophoneTrack = false; // Disable publishing microphone audio
        options.publishCustomAudioTrack = true; // Enable publishing custom audio
        options.enableAudioRecordingOrPlayout = true;
        RtcEngine.SetExternalAudioSource(true, SAMPLE_RATE, SAMPLE_NUM_OF_CHANNEL, 1);

    }
    ```

1. **Open the audio file**

    When the <Vpl k="CLIENT" /> starts, you open the audio file. To implement this workflow, add the following lines at the bottom of the `SetupVideoSDKEngine` method:

    ```csharp
    setExternalVideoSource();
    ```

1. **Start the task to push audio frames**

    When a user successfully joins a channel, you start the task that pushes audio frames. To do this, add the following lines at the bottom of the `onJoinChannelSuccess` callback in the `MainActivity` class:

    ```csharp
    private void PushAudioFrameThread()
    {
        var bytesPerSample = 2;
        var type = AUDIO_FRAME_TYPE.FRAME_TYPE_PCM16;
        var channels = CHANNEL;
        var samples = SAMPLE_RATE / PUSH_FREQ_PER_SEC;
        var samplesPerSec = SAMPLE_RATE;
        var buffer = new byte[samples * bytesPerSample * CHANNEL];
        var freq = 1000 / PUSH_FREQ_PER_SEC;
        var tic = DateTime.Now;
        IntPtr audioFrameBuffer = Marshal.AllocHGlobal(buffer.Length);
        var audioFrame = new AudioFrame
        {
            bytesPerSample = BYTES_PER_SAMPLE.TWO_BYTES_PER_SAMPLE,
            type = type,
            samplesPerChannel = samples,
            samplesPerSec = samplesPerSec,
            channels = channels,
            buffer = (UInt64)audioFrameBuffer,
            bufferPtr = audioFrameBuffer,
            RawBuffer = buffer,
            renderTimeMs = freq
        };
        while (true)
        {
            lock (_pushAudioFrameThreadSignal)
            {
                if (RtcEngine == null)
                {
                    break;
                }
                var toc = DateTime.Now;

                if ((toc - tic).Milliseconds >= freq)
                {
                    lock (_audioBuffer)
                    {
                        if (_audioBuffer.Size > samples * bytesPerSample * CHANNEL)
                        {
                            for (var j = 0; j < samples * bytesPerSample * CHANNEL; j++)
                            {
                                buffer[j] = _audioBuffer.Get();
                            }
                            Marshal.Copy(buffer, 0, audioFrame.bufferPtr, buffer.Length);
                            var ret = RtcEngine.PushAudioFrame(MEDIA_SOURCE_TYPE.AUDIO_PLAYOUT_SOURCE, audioFrame);
                            Debug.Log("PushAudioFrame returns: " + ret);
                            tic = toc;
                        }
                        else
                        {
                            tic = tic.AddMilliseconds(1);
                        }
                    }
                }
            }
            Thread.Sleep(1);
        }
         Marshal.FreeHGlobal(audioFrameBuffer);
    }
    ```

1. **Close the audio file**

    When the <Vpl k="CLIENT" /> is closed, you close the audio file. To do this, add the following lines at the bottom of the `onDestroy` method:

    ```java
    try {
        inputStream.close();
    } catch (IOException e) {
        e.printStackTrace();
    }
    ```

</PlatformWrapper>

---
title: OpenAI Realtime API
sidebar_position: 2
platform_selector: false
description: >
 Integrate OpenAI Realtime MLLM into Conversational AI Engine.
---

import ParameterList, { Parameter } from '@site/src/components/rest-api/ParameterList';

OpenAI Realtime provides multimodal large language model capabilities with real-time audio processing, enabling natural voice conversations without separate ASR/TTS components.

### Enable MLLM

To enable MLLM functionality, set `enable_mllm` to `true` under `advanced_features`.

```json
"advanced_features": {
  "enable_mllm": true
}
```

### Sample configuration

The following example shows a starting `mllm` parameter configuration you can use when you [Start a conversational AI agent](../../rest-api/join).

```json
"mllm": {
  "url": "wss://api.openai.com/v1/realtime",
  "api_key": "<openai_api_key>",
  "params": {
    "model": "gpt-4o-realtime-preview",
    "voice": "coral",
    "instructions": "You are a Conversational AI Agent, developed by Agora.",
    "input_audio_transcription": {
      "language": "<language>",
      "model": "gpt-4o-mini-transcribe",
      "prompt": "expect words related to real-time engagement"
    }
  },
  "max_history": 50,
  "greeting_message": "<greetings>",
  "output_modalities": ["text", "audio"],
  "vendor": "openai",
  "style": "openai"
}
```

### Key parameters

<ParameterList title="mllm" required={true}>
  <Parameter name="url" type="string" required={true}>  
  The WebSocket URL for OpenAI Realtime API. 
  </Parameter>
  <Parameter name="api_key" type="string" required={true}>
  The API key used for authentication. Get your API key from the [OpenAI Console](https://platform.openai.com/api-keys).
  </Parameter>
  <Parameter name="messages" type="array[object]" required={false}>
  Array of conversation items used for short-term memory management. Uses the same structure as `item.content` from the <a href="https://platform.openai.com/docs/api-reference/realtime-client-events/conversation/item/create">OpenAI Realtime API</a>.
  </Parameter>
  <Parameter name="params" type="object" required={false}>
    Additional MLLM configuration parameters.
    - **Modalities override**: The `modalities` setting in params is overridden by `input_modalities` and `output_modalities`.
    - **Turn detection override**: The `turn_detection` setting in params is overridden by the `turn_detection` section outside of `mllm`.  
    See [MLLM Overview](overview) for details.
    <Parameter name="model" type="string" required={false}>
    The model identifier. 
    </Parameter>  
    <Parameter name="voice" type="string" required={false}>
    The voice identifier for audio output. 
    </Parameter>
    <Parameter name="instructions" type="string" required={false}>
    System instructions that define the assistant's behavior and personality.
    </Parameter>
    <Parameter name="input_audio_transcription" type="object" required={false}>
    Configuration for audio input transcription.
      <Parameter name="language" type="string" required={false}>
      The language of the input audio. Supplying the input language in ISO-639-1 format (For example `en`) improves accuracy and latency.
      </Parameter>
      <Parameter name="model" type="string" required={false}>
      The model to use for transcription. Current options are `gpt-4o-transcribe`, `gpt-4o-mini-transcribe`, and `whisper-1`.
      </Parameter>
      <Parameter name="prompt" type="string" required={false}>
      An optional text to guide the model's style or continue a previous audio segment. For `whisper-1`, the prompt is a list of keywords. For `gpt-4o-transcribe` models, the prompt is a free text string, for example "expect words related to technology".
      </Parameter>
    </Parameter>
  </Parameter>
  <Parameter name="max_history" type="integer" defaultValue="32" required={false}>
  The number of conversation history messages to maintain. Cannot exceed the model's context window.
  </Parameter>
  <Parameter name="input_modalities" type="array[string]" defaultValue='["audio"]' required={false}>
    MLLM input modalities:
    - `["audio"]`: Audio only
    - `["audio", "text"]`: Audio plus text
  </Parameter>  
  <Parameter name="output_modalities" type="array[string]" defaultValue='["text", "audio"]' required={false}>
  Output format options: `["text", "audio"]` for both text and voice responses.
  </Parameter>
  <Parameter name="greeting_message" type="string" required={false}>
  Initial message the agent speaks when a user joins the channel.
  </Parameter>  
  <Parameter name="vendor" type="string" required={false}>
  MLLM provider identifier. Set to `openai` for OpenAI Realtime API.
  </Parameter>
  <Parameter name="style" type="string" required={false}>
  API request style. Set to `openai` for OpenAI Realtime API format.
  </Parameter>
</ParameterList>

For comprehensive API reference, real-time capabilities, and detailed parameter descriptions, see the [OpenAI Realtime API documentation](https://platform.openai.com/docs/guides/realtime).

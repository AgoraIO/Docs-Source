---
title: Overview
sidebar_position: 1
platform_selector: false
description: >
  Integrate a custom LLM into Agora's Conversational AI Engine.
---

Large Language Models (LLMs) power the conversational intelligence in your AI agents. They process user input, invoke tools, and generate responses. Agora supports multiple LLM providers, allowing you to choose the best model for your specific requirements.

## Integration steps

To integrate the LLM of your choice, follow these steps:

1. Choose your LLM provider from the [Supported LLMs](#supported-llms) table
1. Obtain an API key from the provider's console
1. Copy the sample configuration for your chosen provider
1. Replace `<api_key>` with your actual API key
1. Customize the `system_messages` for your use case
1. Specify the configuration in the request body as `properties` > `llm` when [Starting a conversational AI agent](/conversational-ai/rest-api/agent/join)


## Supported LLMs

<Vpd k="NAME" /> currently supports the following LLMs:

- [OpenAI](openai)
- [Azure OpenAI](azure-openai)
- [Groq](groq)
- [Google Gemini](gemini)
- [Google Vertex AI](google-vertex-ai)
- [Claude Anthropic](claude)
- [Amazon Bedrock](amazon)
- [Dify](dify)
- [Custom LLM](#custom-llms)

## Custom LLMs

You can integrate any LLM that provides an OpenAI-compatible REST API interface. Your custom service must handle requests and responses in the OpenAI API format. For implementation details and requirements, see the [Custom LLM](../../develop/custom-llm) integration guide.

import ImportLibrary from '@docs/assets/code/video-sdk/custom-video-and-audio/import-library.mdx';
import SetVariables from '@docs/assets/code/video-sdk/custom-video-and-audio/set-variables.mdx';
import EnableVideoPublishing from '@docs/assets/code/video-sdk/custom-video-and-audio/enable-video-publishing.mdx';
import PushVideoFrames from '@docs/assets/code/video-sdk/custom-video-and-audio/push-video-frames.mdx';
import EnableAudioPublishing from '@docs/assets/code/video-sdk/custom-video-and-audio/enable-audio-publishing.mdx';
import PushAudioFrames from '@docs/assets/code/video-sdk/custom-video-and-audio/push-audio-frames.mdx';


<PlatformWrapper platform="ios">
<ProductWrapper notAllowed="voice-calling">
### Implement a custom video source

1.  **Add the required imports**

    <ImportLibrary />

1.  **Add the required variables**

    <SetVariables />

1.  **Enable custom video track publishing**

    When a user presses **Join**, you configure the following to enable publishing of the captured video from a custom source:

    <PlatformWrapper platform="ios">
    Add the following line to the `init()` method of `AgoraManager` to set the external video source to `true` before joining a channel:
    </PlatformWrapper>

    <EnableVideoPublishing />

1.  **Setup a video renderer for custom video**

    <PlatformWrapper platform="ios">
    <Vg k="VSDK" /> does not support rendering video frames captured in the push mode. You need to implement a custom video renderer using methods from outside the SDK.

    For this, [`AVCaptureDevice`](https://developer.apple.com/documentation/avfoundation/avcapturedevice) and [`AVCaptureSession`](https://developer.apple.com/documentation/avfoundation/avcapturesession) can be used to capture frames and manage capturing sessions.

    Have a look at [`CustomAudioVideoView`](https://github.com/AgoraIO/video-sdk-samples-ios/blob/main/custom-video-and-audio/CustomAudioVideoView.swift) for more details.
    </PlatformWrapper>

1.  **Push the video frames**

    A callback method of `AgoraManager` is called when a video frame is captured. You use the data in the callback to create a `VideoFrame`, and push the `VideoFrame` to the channel.

    <PushVideoFrames />
</ProductWrapper>

### Implement a custom audio source

1.  **Enable publishing the custom audio**

    When a user presses **Join**, you configure the following to enable publishing of the captured audio from a custom source:

    <PlatformWrapper platform="ios">
    Add the following line to the `init()` method of `AgoraManager` to set the external audio source to `true` before joining a channel:
    </PlatformWrapper>

    <EnableAudioPublishing />

1.  **Setup an audio renderer for custom audio**

    <PlatformWrapper platform="ios">
    For rendering audio frames captured in the push mode, implement a custom audio renderer using methods from outside the SDK because it's not supported by <Vg k="VSDK" />.

    [AVCaptureAudioDataOutput](https://developer.apple.com/documentation/avfoundation/avcaptureaudiodataoutput) can be used to receive the audio frames when they are captured. These frames can then be added as an audio output to an audio capturing session using [`AVCaptureSession`](https://developer.apple.com/documentation/avfoundation/avcapturesession).

    Have a look at [`AgoraAudioSourcePush`](https://github.com/AgoraIO/video-sdk-samples-ios/blob/main/custom-video-and-audio/AgoraAudioSourcePush.swift) for more details.
    </PlatformWrapper>

1.  **Push the audio frames**

    When an audio frame is captured, a callback method of `AgoraManager` is called. You push this data in the callback to the <Vg k="VSDK" /> to use as an external audio frame sample buffer:

    <PushAudioFrames />

</PlatformWrapper>
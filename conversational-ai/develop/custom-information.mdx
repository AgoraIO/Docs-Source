---
title: Transmit custom information
sidebar_position: 3
platform_selector: false
description: >
   Transmit custom information to the Conversational AI agent to guide it in generating responses customized for the user.
---

When interacting with a <Vpd k="AGENT" />, you can transmit custom context information from the client, such as the user's speaking status, selected text, personal signature, or score, enabling the agent to generate responses tailored to the user's needs.

This document explains how to use the capabilities of the [Signaling SDK](/signaling/overview/product-overview) to include custom information in interactions with the <Vpd k="AGENT" />.

## Understand the tech
[Agora Signaling SDK](/signaling/overview/product-overview) allows users in a channel to set custom temporary status information and notifies other online users in the channel through event notifications.

If your app integrates both <Vg k="ASDK" /> and <Vg k="SIG" /> services, you can leverage the <Vg k="SIG" /> features when creating a <Vpd k="AGENT" />. This allows the agent to retrieve temporary status information from the <Vg k="SIG" /> channel before invoking the LLM. The information is used as context to guide the agent in generating responses that better align with user needs.

## Prerequisites

Before you begin, ensure that you have:

- Implemented the basic logic for interacting with a <Vpd k="AGENT" /> by following the [REST Quickstart](../get-started/quickstart).
- Integrated the Signaling SDK into your app and implemented basic messaging functionality by following the [<Vg k="SIG" /> SDK Quickstart](/signaling/get-started/sdk-quickstart).

## Implementation
Take the following steps to transmit custom context information from the client to the LLM.

### Enable Signaling

To enable <Vg k="SIG" /> integration, set `advanced_features.enable_rtm` to `true`in the [POST request](/conversational-ai/rest-api/agent/join) when creating a <Vpd k="AGENT" />. Refer to the following sample request:

```bash
curl --request post \
  --url https://api.agora.io/api/conversational-ai-agent/v2/projects/<your_app_id>/join \
  --header 'Authorization: Basic <credentials>' \
  --data '
{
  "name": "unique_name",
  "properties": {
    "channel": "channel_name",
    "token": "token",
    "agent_rtc_uid": "friday",
    "agent_rtm_uid": "friday",
    "advanced_features": {
      "enable_rtm": true
    },
    "remote_rtc_uids": [
      "*"
    ],
    "enable_string_uid": true,
    "llm": {
      "url": "https://api.openai.com/v1/chat/completions",
      "api_key": "<your_llm_key>",
      "params": {
        "model": "gpt-4o-mini"
      }
    },
    "tts": {
      "vendor": "microsoft",
      "params": {
        "key": "<your_tts_api_key>",
        "voice_id": "en-US-AndrewMultilingualNeural"
      }
    }
  }
}'
```

### Set Custom Information
Refer to <Vg k="SIG" /> [User status management](/signaling/core-functionality/presence#user-status-management) to set status information for users in the channel.

### Transmit Custom Information
Before invoking the LLM, the agent automatically retrieves the active user's temporary status information and transmits it as context to the model. This temporary status information is stored in the `context.presence` field.

The following example illustrates a scenario where UserA selects the text "Pythagorean theorem" in the app and asks the agent, "What does this mean?". The JSON example shows how the agent retrieves a temporary status field named `selection` from Signaling before calling the LLM. The request structure is as follows:

```json
{
  "messages": [
    {
      "role": "user",
      "content": "What does this mean?"
    }
  ],
  "context": {
    "presence": {
      "userA": {
        "selection": "Pythagorean Theorem"
      }
    }
  },
  "model": "openai"
}
```

### LLM Adaptation

Adapt the LLM to process the temporary status information in the `context.presence` field and generate content that better meets user needs. For implementation details, refer to the [Custom LLM](../develop/custom-llm) documentation.
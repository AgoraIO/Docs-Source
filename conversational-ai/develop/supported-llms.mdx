---
title: Supported LLMs
sidebar_position: 0.5
platform_selector: false
description: >
  Integrate a custom LLM into Agora's Conversational AI Engine.
---

A large-scale language model (LLM) is an artificial intelligence model that can generate textual output from textual input. In speech AI applications, they lie between speech-to-text (STT) and text-to-speech (TTS) and are responsible for tool invocation and generating text responses from agents.

## Supported LLMs

<Vpd k="NAME" /> currently supports the following LLMs:

| Provider         | Style       | Documentation     |
|------------------|-------------|-------------------|
| OpenAI           | `openai`      | https://platform.openai.com/docs/api-reference/responses/create    |
| Azure OpenAI     | `openai`      | https://learn.microsoft.com/en-us/azure/ai-services/openai/reference              |
| Google Gemini    | `gemini`      | https://cloud.google.com/docs/authentication/rest   |
| Google Vertex AI | `gemini`      | https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/system-instructions |
| Claude           | `anthropic`   | https://docs.anthropic.com/en/api/messages          |
| Dify             | `dify`        | [Agora Conversational AI - Dify Marketplace](https://marketplace.dify.ai/plugins/plutoless/convoai)          |
| Custom LLM       | `openai`      | https://docs.agora.io/en/conversational-ai/develop/custom-llm      |

### OpenAI

OpenAI provides state-of-the-art language models including GPT-4o, GPT-4o-mini, and other variants optimized for different use cases.

#### Authentication

To connect to OpenAI LLM, you need an API key for authentication.

#### Sample configuration

The following example shows a starting `llm` parameter configuration you can use when you [Start a conversational AI agent](../rest-api/join).

```json
"llm": {
    "url": "https://api.openai.com/v1/chat/completions",
    "api_key": "<your_llm_key>",
    "system_messages": [
      {
        "role": "system",
        "content": "You are a helpful chatbot."
      }
    ],
    "max_history": 32,
    "greeting_message": "Hello, how can I assist you",
    "failure_message": "Please hold on a second.",
    "params": {
      "model": "gpt-4o-mini"
    }
  }
```

#### Key parameters

-  `api_key`: Create or manage your API key from [OpenAI organization settings](https://platform.openai.com/settings/organization/api-keys).
- `url`: Use the completions endpoint.
- `model`: Refer to [OpenAI models](https://platform.openai.com/docs/models) for available models.

For advanced configuration options, model capabilities, and detailed parameter descriptions, see the [OpenAI API documentation](https://platform.openai.com/docs/api-reference/responses/create).

### Azure OpenAI

Azure OpenAI service provides REST API access to OpenAI's powerful language models through Microsoft Azure's secure infrastructure.

#### Authentication

To connect to Azure OpenAI, you need an API key for authentication.

#### Sample configuration

The following example shows a starting `llm` parameter configuration you can use when you [Start a conversational AI agent](../rest-api/join).

```json
"llm": {
    "url": "https://YOUR_RESOURCE_NAME.openai.azure.com/openai/deployments/YOUR_DEPLOYMENT_NAME/chat/completions?api-version=2024-06-01",
    "api_key": "<api_key>",
    "system_messages": [
        {
            "role": "system",
            "content": "You are a helpful chatbot."
        }
    ],
    "max_history": 32,
    "greeting_message": "Hello, how can I assist you today?",
    "failure_message": "Please hold on a second.",
    "params": {
        "model": "ChatGPT-4o-min"
    }
}
```

#### Key parameters

- `api_key`: Find your API key in the Azure portal under your OpenAI resource.
- `url`: Replace `YOUR_RESOURCE_NAME` with your Azure resource name and `YOUR_DEPLOYMENT_NAME` with your model deployment name.
- `model`: Use the deployment name you created in Azure (not the base model name).

For advanced configuration options, deployment setup, and detailed parameter descriptions, see the [Azure OpenAI API documentation](https://learn.microsoft.com/en-us/azure/ai-services/openai/reference).


### Google Gemini

Google Gemini provides advanced multimodal AI capabilities with fast performance and efficient processing for conversational AI applications.

#### Authentication

To connect to Google Gemini, you need an API key for authentication.

#### Sample configuration

The following example shows a starting `llm` parameter configuration you can use when you [Start a conversational AI agent](../rest-api/join).

```json
"llm": {
   "url": "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:streamGenerateContent?alt=sse&key=<api_key>",
   "system_messages": [
       {
           "parts": [
               {
                   "text": "You are a helpful chatbot"
               }
           ],
           "role": "user"
       }
   ],
   "max_history": 32,
   "greeting_message": "Good to see you!",
   "failure_message": "Hold on a second.",
   "params": {
       "model": "gemini-2.0-flash" 
   },
   "ignore_empty": true,
   "style": "gemini"
}
```

#### Key parameters

- `url`: Note that the API key is passed in the URL query parameter. Get your API key from [Google AI Studio](https://aistudio.google.com/app/apikey).
- `system_messages`: Use `parts` array with `text` objects instead of simple `content` string.
- `model`: Refer to [Gemini models](https://ai.google.dev/gemini-api/docs/models/gemini) for available models.
- `style`: Set to `"gemini"` to use Gemini's message format.
- `ignore_empty`: Set to `true` to handle empty responses appropriately.

For advanced configuration options, model capabilities, and detailed parameter descriptions, see the [Google Gemini API documentation](https://cloud.google.com/docs/authentication/rest).

### Google Vertex AI

Google Vertex AI provides enterprise-grade access to Google's generative AI models with enhanced security, scaling capabilities, and integration with Google Cloud services.

#### Authentication

To connect to Google Vertex AI, you need an API key for authentication.

#### Sample configuration

The following example shows a starting `llm` parameter configuration you can use when you [Start a conversational AI agent](../rest-api/join).

```json
"llm": {
   "url": "https://us-central1-aiplatform.googleapis.com/v1/projects/inbound-coast-460602-g1/locations/us-central1/publishers/google/models/gemini-2.0-flash-001:streamGenerateContent?alt=sse",
   "api_key": "<api_key>",
   "system_messages": [
       {
           "role": "user",
           "parts": [ {"text": "You are a helpful chatbot."} ]
       }
   ],
   "max_history": 32,
   "greeting_message": "Good to see you!",
   "failure_message": "Hold on a second.",
   "params": {
       "model": "gemini-2.0-flash-001"
   },
   "style": "gemini"
}
```

#### Key parameters

- `api_key`: Get your API key from Google Cloud Console with Vertex AI API enabled.
- `url`: Use Vertex AI endpoint and includes your Google Cloud project ID and region in the URL path.
- `model`: Refer to [Vertex AI models](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models) for available models.
- `system_messages`: Use `parts` array with `text` objects instead of simple `content` string.
- `style`: Set to `"gemini"` to use Gemini's message format.

For advanced configuration options, model capabilities, and detailed parameter descriptions, see the [Google Vertex AI API documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/system-instructions).

### Claude

Claude from Anthropic provides helpful, harmless, and honest AI assistance with strong reasoning capabilities and natural conversational abilities.

#### Authentication

To connect to Claude, you need an API key for authentication.

#### Sample configuration

The following example shows a starting `llm` parameter configuration you can use when you [Start a conversational AI agent](../rest-api/join).

```json
"llm": {
   "url": "https://api.anthropic.com/v1/messages",
   "api_key": "<api_key>",
   "headers": "{\"anthropic-version\":\"2023-06-01\"}",
   "system_messages": [
       {
           "role": "user",
           "content": "You are a Conversational AI Agent, developed by Agora."
       }
   ],
   "max_history": 2,
   "greeting_message": "Good to see you!",
   "failure_message": "Hold on a second.",
   "params": {
       "model": "claude-3-7-sonnet-20250219",
       "max_tokens": 1024
   },
   "style": "anthropic",
   "ignore_empty": true
}
```

#### Key parameters

- `api_key`: Get your API key from [Anthropic Console](https://console.anthropic.com/).
- `url`: Use Anthropic's messages endpoint.
- `headers`: Must include `anthropic-version` header for API compatibility.
- `model`: Refer to [Claude models](https://docs.anthropic.com/en/docs/about-claude/models) for available models.
- `max_tokens`: Must specify maximum tokens in the response.
- `style`: Set to `"anthropic"` to use Claude's message format.
- `ignore_empty`: Set to `true` to handle empty responses appropriately.

For advanced configuration options, model capabilities, and detailed parameter descriptions, see the [Claude API documentation](https://docs.anthropic.com/en/api/messages).

### Dify

Dify is an open-source LLM application development platform that provides a unified API for multiple LLM providers with workflow orchestration and advanced features.

#### Authentication

To connect to Dify, you need an API key for authentication.

#### Sample configuration

The following example shows a starting `llm` parameter configuration you can use when you [Start a conversational AI agent](../rest-api/join).

```json
"llm": {
   "url": "https://qv90***.ai-plugin.io/convoai-start",
   "api_key": "<same_api_key_as_in_endpoint_setup>",
   "system_messages": [
       {
           "role": "system",
           "content": "You are a helpful voice assistant."
       }
   ],
   "max_history": 32,
   "greeting_message": "Hello, how can I assist you today?",
   "failure_message": "Hold on a moment.",
   "params": {
       "model": "default"
   },
   "style": "dify"
}
```

#### Key parameters

- `api_key`: Use the same API key configured in your Dify endpoint setup.
- `url`: Use the custom endpoint URL from your Dify marketplace integration.
- `model`: Set to `"default"` to use the model configured in your Dify application.
- `style`: Set to `"dify"` to use Dify's message format and workflow processing.

For advanced configuration options, workflow setup, and detailed parameter descriptions, see the [Agora Conversational AI - Dify Marketplace](https://marketplace.dify.ai/plugins/plutoless/convoai) documentation.

### Custom LLMs

To integrate a custom LLM, you need to provide an HTTP service compatible with the OpenAI API, capable of handling requests and responses in the OpenAI API format. Refer to [Custom LLM](../develop/custom-llm) for details.

<PlatformWrapper platform="windows">

### Implement the user interface

To enable or disable processing of captured raw video data, add a button to the user interface:

    1. Go to menu **View** > **Other Windows** > **Resource View**.
    
        If the **Resource View** window isn't the top-most window, select the **Resource View** tab to bring it to the top.
    
    1. In **Resource View**, go to **AgoraImplementation.rc** > **Dialog** and double-click `IDD_AGORAIMPLEMENTATION_DIALOG`.
        
        The resource opens inside the **Dialog Editor**. 

    1. Click **View** > **ToolBox**. The **Toolbox** opens.

    1. From **ToolBox**, drag **Button** to the surface of the **Dialog Editor**.

    1. In **Properties**, update the **Caption** field to `Zoom In`.

### Handle the system logic

This sections describes the steps required to use the relevant libraries and declare the necessary variables, and set up access to the UI elements.

1.  **Define a variable to manage video processing**

        In **Solution Explorer**, open `AgoraImplementationDlg.cpp` and add the following variable after the list of header includes:
        
        ```cpp
        bool isZoomed = false;
        ```
2.  **Start and stop video processing**
    
    When the user presses **Zoom In**, update the state of `isZoomed`. This variable
  controls the behavior of the `onCaptureVideoFrame` callback to enable or disable frame cropping. To implement
  this workflow:

    1. In **Dialog Editor**, double-click **Zoom In**; this creates an event listener.
    1. Add the following code to the event listener:
        ```cpp
        // Toggle the zoom state and get a handle to the button control
        isZoomed = !isZoomed;
        // We implemented the Agora Windows Get Started guide, using the button with ID 'IDC_BUTTON3' for zoom in and zoom out functionality. However, please note that the button ID may differ in your specific scenario.
        CButton* pButtonCtrl = static_cast<CButton*>(GetDlgItem(IDC_BUTTON3));

        // Set the button text based on the zoom state
        pButtonCtrl->SetWindowText(isZoomed ? L"Zoom Out" : L"Zoom In");
        ```

### Implement processing of raw video and audio data

<Vg k = "COMPANY"/> provides the `IVideoFrameObserver` and `IAudioFrameObserver` interfaces for real-time video and
  audio processing. These interfaces enable developers to capture and process raw video and audio data before it is sent
  over the network. To implement these interfaces:

1.  **Implement the video frame observer**
    
    `IVideoFrameObserver` provides access to each local video frame after it is captured, as well as to each remote
  video frame before it is played back. In this sample, the `onCaptureVideoFrame` callback is used to modify the
  captured video frame buffer for cropping and scaling, resulting in a modified version of the frame that is played back.

    To implement this workflow, in `AgoraImplementation.cpp`, add the following class:
      ```cpp
      class CustomVideoFrameObserver : public IVideoFrameObserver
      {
          // IVideoFrameObserver Implementation
          public:
          CustomVideoFrameObserver() : originalYBuffer(nullptr), originalUBuffer(nullptr), originalVBuffer(nullptr) {}
          ~CustomVideoFrameObserver()
          {
              delete[] originalYBuffer;
              delete[] originalUBuffer;
              delete[] originalVBuffer;
          }
        // This callback is triggered by the SDK when a video frame is captured. It processes the raw video frame data, crops the frame if necessary, and returns a boolean value to indicate whether the pre-processing was successful.
        bool CustomVideoFrameObserver::onCaptureVideoFrame(VideoFrame& videoFrame) override
        {
          // Check if zooming is enabled
          if (isZoomed)
          {
            // Store the original width and height
            int originalWidth = videoFrame.width;
            int originalHeight = videoFrame.height;

            // Calculate the new width and height after zooming (in this case, halving the dimensions)
            int newWidth = originalWidth / 2;
            int newYStride = newWidth;
            int newHeight = originalHeight / 2;

            // Create temporary buffers for the cropped frame data
            uint8_t* newYBuffer = new uint8_t[newWidth * newHeight];
            uint8_t* newUBuffer = new uint8_t[newWidth * newHeight / 4];
            uint8_t* newVBuffer = new uint8_t[newWidth * newHeight / 4];

            // Loop over the new frame data and copy the appropriate pixels from the original frame
            for (int y = 0; y < newHeight; ++y)
            {
              for (int x = 0; x < newWidth; ++x)
              {
                // Copy Y plane
                newYBuffer[y * newYStride + x] = videoFrame.yBuffer[y * videoFrame.yStride + x];

                // Copy U and V planes for every 2x2 block of pixels
                if (y % 2 == 0 && x % 2 == 0)
                {
                  int newIndexUV = (y / 2) * (newWidth / 2) + (x / 2);
                  int originalIndexUV = (y / 2) * (videoFrame.uStride) + (x / 2);

                  newUBuffer[newIndexUV] = videoFrame.uBuffer[originalIndexUV];
                  newVBuffer[newIndexUV] = videoFrame.vBuffer[originalIndexUV];
                }
              }
            }

            // Update videoFrame with the new width, height, and strides
            videoFrame.width = newWidth;
            videoFrame.height = newHeight;
            videoFrame.yStride = newYStride;
            videoFrame.uStride = newWidth / 2;
            videoFrame.vStride = newWidth / 2;

            // Check if the original buffers have already been stored
            if (!originalYBuffer)
            {
              originalYBuffer = videoFrame.yBuffer;
              originalUBuffer = videoFrame.uBuffer;
              originalVBuffer = videoFrame.vBuffer;
            }

            // Update videoFrame to reference the new buffers
            videoFrame.yBuffer = newYBuffer;
            videoFrame.uBuffer = newUBuffer;
            videoFrame.vBuffer = newVBuffer;
          }

          // Return true to indicate that pre-processing is successful and the frame should not be ignored
          return true;
        }
        // Note that `getVideoFrameProcessMode()` is need to override with 'PROCESS_MODE_READ_WRITE' to get updated video frames
        IVideoFrameObserver::VIDEO_FRAME_PROCESS_MODE getVideoFrameProcessMode()
        {
          return VIDEO_FRAME_PROCESS_MODE::PROCESS_MODE_READ_WRITE;
        }

        // Rest the below functions are dummy "do-nothing" implentaion just to sake for overriding of interface pure virtual functions.
        bool onSecondaryCameraCaptureVideoFrame(VideoFrame& videoFrame)
        {
          return false;
        }

        bool onSecondaryPreEncodeCameraVideoFrame(VideoFrame& videoFrame)
        {
          return false;
        }

        bool onScreenCaptureVideoFrame(VideoFrame& videoFrame)
        {
          return false;
        }

        bool onPreEncodeScreenVideoFrame(VideoFrame& videoFrame)
        {
          return false;
        }

        bool onSecondaryScreenCaptureVideoFrame(VideoFrame& videoFrame)
        {
          return false;
        }

        bool CustomVideoFrameObserver::onSecondaryPreEncodeScreenVideoFrame(VideoFrame& videoFrame)
        {
          return false;
        }

        bool onRenderVideoFrame(const char* channelId, rtc::uid_t remoteUid, VideoFrame& videoFrame)
        {
          return false;
        }

        bool onPreEncodeVideoFrame(VideoFrame& videoFrame)
        {
          return false;
        }

        bool onMediaPlayerVideoFrame(VideoFrame& videoFrame, int mediaPlayerId)
        {
          return false;
        }

        bool onTranscodedVideoFrame(VideoFrame& videoFrame)
        {
          return false;
        }

          private:
        uint8_t* originalYBuffer;
        uint8_t* originalUBuffer;
        uint8_t* originalVBuffer;
      };
      ```
    You must override `getVideoFrameProcessMode()` with `PROCESS_MODE_READ_WRITE` to ensure that any modifications made to the raw data take effect.

2. **Implement audio frame observer callbacks**
            
    `IAudioFrameObserver` provides access to each captured or played back audio frame, allowing you to process the audio frames according to your needs.

    To override `IAudioFrameObserver` callbacks, add the following to `AgoraImplementationDlg.cpp`:
    ```cpp
    class CustomAudioFrameObserver : public IAudioFrameObserver 
    {
        // IAudioFrameObserver Implementation
        // User can provide implementation for "onRecordAudioFrame()", "onPlaybackAudioFrame()" and "onMixedAudioFrame()" as per scenario. 
        bool onRecordAudioFrame(const char* channelId, AudioFrame& audioFrame)
        {
            // Gets the captured audio frame.
            // Add code here to process the recorded audio.
            return true;
        }

	    bool onPlaybackAudioFrame(const char* channelId, AudioFrame& audioFrame)
        {
            // Gets the audio frame for playback.
            // Add code here to process the playback audio.
		    return true;
        }
        bool onMixedAudioFrame(const char* channelId, AudioFrame& audioFrame)
        {
            // Retrieves the mixed captured and playback audio frame.
            // Add code here to process the mixed captured playback audio
            return true;
        }

	    // Rest the below functions are dummy "do-nothing" implementation just to sake for overriding of interface pure virtual functions.
	    bool onPlaybackAudioFrameBeforeMixing(const char* channelId, rtc::uid_t uid, AudioFrame& audioFrame)
	    {
            return false;
        }

	    bool onEarMonitoringAudioFrame(AudioFrame& audioFrame)
        {
            return false;
        }

	    int getObservedAudioFramePosition()
        {
            return 0;
        }

	    IAudioFrameObserver::AudioParams getPlaybackAudioParams()
        {
            return AudioParams();
        }

	    IAudioFrameObserver::AudioParams getRecordAudioParams()
        {
            return AudioParams();
        }

	    IAudioFrameObserver::AudioParams getMixedAudioParams()
        {
            return AudioParams();
        }

	    IAudioFrameObserver::AudioParams getEarMonitoringAudioParams()
        {
            return AudioParams();
        }
    };    
    ```

2.  **Register the video and audio frame observers**

    To receive callbacks declared in `IVideoFrameObserver` and `IAudioFrameObserver`, you register the video and audio frame observers with <Vg k="ENGINE" /> before joining a channel. To specify the format of audio frames captured by each `IAudioFrameObserver` callback, use the `setRecordingAudioFrameParameters`, `setPlaybackAudioFrameParameters` and `setMixedAudioFrameParameters` methods.

    To register the observers, do the following:

    1. Setup a function to register and unregister observers for audio and video frames. In `AgoraImplementationDlg.h`, add the following code after `void setupVideoSDKEngine();`:
      
        ```cpp
        bool EnableAudioVideoCapture(bool bEnable);
        ```
    
    1. In `AgoraImplementationDlg.cpp`, add the following method before `CAboutDlg()`:

        ```cpp
        bool CAgoraImplementationDlg::EnableAudioVideoCapture(bool bEnable)
        {
            agora::util::AutoPtr<agora::media::IMediaEngine> mediaEngine;
            //query interface agora::AGORA_IID_MEDIA_ENGINE in the engine.
            mediaEngine.queryInterface(agoraEngine, agora::rtc::AGORA_IID_MEDIA_ENGINE);
            int nRet = 0;
            agora::base::AParameter apm(agoraEngine);
            if (mediaEngine.get() == NULL) return FALSE;
            if (bEnable) 
            {
                // Register video & audio frame observer
                nRet = mediaEngine->registerVideoFrameObserver(new CustomVideoFrameObserver());
                nRet = mediaEngine->registerAudioFrameObserver(new CustomAudioFrameObserver());

		        // Set the format of the captured raw audio data.
		        int SAMPLE_RATE = 16000, SAMPLE_NUM_OF_CHANNEL = 1, SAMPLES_PER_CALL = 1024;

		        agoraEngine->setRecordingAudioFrameParameters(SAMPLE_RATE, SAMPLE_NUM_OF_CHANNEL,
                RAW_AUDIO_FRAME_OP_MODE_READ_WRITE, SAMPLES_PER_CALL);

		        agoraEngine->setPlaybackAudioFrameParameters(SAMPLE_RATE, SAMPLE_NUM_OF_CHANNEL,
                RAW_AUDIO_FRAME_OP_MODE_READ_WRITE, SAMPLES_PER_CALL);

		        agoraEngine->setMixedAudioFrameParameters(SAMPLE_RATE, SAMPLE_NUM_OF_CHANNEL, SAMPLES_PER_CALL);
            }
            else 
            {
                nRet = mediaEngine->registerVideoFrameObserver(NULL);
                nRet = mediaEngine->registerAudioFrameObserver(NULL);
            }
            return nRet == 0 ? TRUE : FALSE;
        }
        ```

    1. To register the observers at startup, in `AgoraImplementationDlg.cpp`, locate `setupVideoSDKEngine()` and add the following before
     `agoraEngine->setClientRole(CLIENT_ROLE_TYPE::CLIENT_ROLE_BROADCASTER);`:

        ```cpp
        // Enable and register observers for audio and video frames
        EnableAudioVideoCapture(TRUE);
        ```
3. **Unregister the video and audio frame observers when you close the <Vpl k="CLIENT"/>**
    
    When you quit the <Vpl k="CLIENT"/>, to unregister the frame observers, set
  `EnableAudioVideoCapture()` to `false`.

    To implement this workflow, in `AgoraImplementationDlg.cpp`, add the following to `OnClose` before `	agoraEngine->release();`:

    ```cpp
    // Unregister observers for audio and video frames
	EnableAudioVideoCapture(FALSE);
    ```
</PlatformWrapper>
---
title: 'Use a Local LLM with Ollama'
sidebar_position: 4
type: docs
description: >
 Use Ollama models locally with Ten Agent by leveraging OpenAI-compatible endpoints.
---

<Vpd k="NAME" /> supports any LLM that provides OpenAI-compatible APIs. This guide shows you how to run models locally using Ollama, eliminating the need for external API calls while maintaining the same interface.

## Prerequisites

Before starting, ensure you have:

- Completed the [TEN Agent quickstart](../get-started/quickstart)
- Sufficient hardware for your chosen model:
    - CPU: Modern multi-core processor
    - RAM: 8GB minimum (16GB+ recommended for larger models)
    - GPU: Optional but recommended for better performance

## Implementation

### Install Ollama

Begin by installing Ollama and downloading your preferred language model:

1. Download and install Ollama from [ollama.com](https://ollama.com/)

2. Download your desired model using `ollama pull {model name}`. For example:
  
    ```bash
    # Run on your local machine
    ollama pull llama3.2
    ```

### Start the Ollama server

Launch the Ollama server with network access enabled to allow Docker containers to connect:

```bash
OLLAMA_HOST=0.0.0.0 ollama serve
```

Since <Vpd k="NAME" /> runs in Docker, you'll need your machine's private IP address:

```bash
# Get your private IP on macOS/Linux
ifconfig | grep "inet " | grep -v 127.0.0.1

# Get your private IP on Windows
ipconfig | findstr "IPv4"
```

### Configure TEN Agent

Connect TEN Agent to your local Ollama instance through the playground interface:

1. Open the playground at http://localhost:3000
1. Select the `voice_assistant` graph
1. In **Module Picker**, Choose **OpenAI** as the LLM provider
1. In Settings, configure the llm extension as follows:

    - **Base URL**: `http://<your-private-ip>:11434/v1/`
    - **Model**: Your downloaded model name. For example, `llama3.2`
    - **API Key**: Any non-empty string. Ollama doesn't require authentication.
    - **Prompt**: Your system prompt. For example, "You're a helpful assistant."

1. Save your configuration in the playground

### Test the integration

Verify your setup by running a test conversation:

1. Start a conversation to test the connection
1. Monitor Ollama logs for requests

## Reference

### OpenAI compatibility

For more information on Ollamaâ€™s OpenAI compatibility, refer to the [official blog post](https://ollama.com/blog/openai-compatibility).

### Troubleshooting

If you encounter issues while setting up Ollama with TEN Agent, refer to these common problems and their solutions:

| Issue | Possible solutions |
|-------|-----------|
| **Connection refused** | <ul><li>Verify Ollama is running with `OLLAMA_HOST=0.0.0.0`</li><li>Check your firewall settings</li><li>Ensure you're using the correct private IP</li></ul> |
| **Model not found** | <ul><li>Confirm the model is downloaded: `ollama list`</li><li>Use the exact model name in your configuration</li></ul> |
| **Performance issues** | <ul><li>Try a smaller model like `mistral`</li><li>Ensure sufficient system resources</li><li>Consider GPU acceleration if available</li></ul> |
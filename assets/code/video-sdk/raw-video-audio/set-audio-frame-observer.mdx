<PlatformWrapper platform="android">
    ```kotlin
    private val iAudioFrameObserver: IAudioFrameObserver = object : IAudioFrameObserver {
        override fun onRecordAudioFrame(
            channelId: String?,
            type: Int,
            samplesPerChannel: Int,
            bytesPerSample: Int,
            channels: Int,
            samplesPerSec: Int,
            buffer: ByteBuffer?,
            renderTimeMs: Long,
            avsync_type: Int
        ): Boolean {
            // Gets the captured audio frame.
            // Add code here to process the recorded audio.
            return false
        }

        override fun onPlaybackAudioFrame(
            channelId: String?,
            type: Int,
            samplesPerChannel: Int,
            bytesPerSample: Int,
            channels: Int,
            samplesPerSec: Int,
            buffer: ByteBuffer?,
            renderTimeMs: Long,
            avsync_type: Int
        ): Boolean {
            // Gets the audio frame for playback.
            // Add code here to process the playback audio.
            // return true to indicate that Data has been processed
            return false
        }

        override fun onMixedAudioFrame(
            channelId: String?,
            type: Int,
            samplesPerChannel: Int,
            bytesPerSample: Int,
            channels: Int,
            samplesPerSec: Int,
            buffer: ByteBuffer?,
            renderTimeMs: Long,
            avsync_type: Int
        ): Boolean {
            // Retrieves the mixed captured and playback audio frame.
            return false
        }

        override fun onEarMonitoringAudioFrame(
            type: Int,
            samplesPerChannel: Int,
            bytesPerSample: Int,
            channels: Int,
            samplesPerSec: Int,
            buffer: ByteBuffer?,
            renderTimeMs: Long,
            avsync_type: Int
        ): Boolean {
            return false
        }

        override fun onPlaybackAudioFrameBeforeMixing(
            channelId: String?,
            userId: Int,
            type: Int,
            samplesPerChannel: Int,
            bytesPerSample: Int,
            channels: Int,
            samplesPerSec: Int,
            buffer: ByteBuffer?,
            renderTimeMs: Long,
            avsync_type: Int
        ): Boolean {
            // Retrieves the audio frame of a specified user before mixing.
            return false
        }

        override fun getObservedAudioFramePosition(): Int {
            return 0
        }

        override fun getRecordAudioParams(): AudioParams {
            return AudioParams(sampleRate,numberOfChannels, 0 ,samplesPerCall)
        }

        override fun getPlaybackAudioParams(): AudioParams {
            return AudioParams(sampleRate,numberOfChannels, 0 ,samplesPerCall)
        }

        override fun getMixedAudioParams(): AudioParams {
            return AudioParams(sampleRate,numberOfChannels, 0 ,samplesPerCall)
        }

        override fun getEarMonitoringAudioParams(): AudioParams {
            return AudioParams(sampleRate,numberOfChannels, 0 ,samplesPerCall)
        }
    }
    ```
    - <Link to = "{{global.API_REF_ANDROID_ROOT}}/class_iaudioframeobserver.html">IAudioFrameObserver</Link>
</PlatformWrapper>
<PlatformWrapper platform="ios">
    `AgoraAudioFrameDelegate` gives you access to each audio frame after it is captured or before it is played back.

    Have a look at [`ModifyAudioFrameDelegate`](https://github.com/AgoraIO/video-sdk-samples-ios/blob/main/stream-raw-audio-and-video/ModifyAudioFrameDelegate.swift) for details on how to use `AgoraAudioFrameDelegate` to set up an audio frame observer.
</PlatformWrapper>

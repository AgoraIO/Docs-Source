
<PlatformWrapper platform="android">

### Implement the user interface

To enable or disable processing of captured raw video data, add a button to the user interface. In `/app/res/layout/activity_main.xml` add the following lines before `</RelativeLayout>`:

```xml
<Button
    android:id="@+id/ZoomButton"
    android:layout_width="wrap_content"
    android:layout_height="wrap_content"
    android:layout_below="@+id/JoinButton"
    android:layout_alignEnd="@id/LeaveButton"
    android:layout_alignStart="@id/JoinButton"
    android:onClick="setZoom"
    android:text="Zoom In" />
```

### Handle the system logic

This sections describes the steps required to use the relevant libraries, declare the necessary variables, and set up access to the UI
elements.

1.  **Import the required Android and Agora libraries**

    To integrate <Vg k="VSDK" /> frame observer libraries into your <Vpl k="CLIENT" /> and access the button object, add the following statements after the last `import` statement in `/app/java/com.example.<projectname>/MainActivity`.

    ```java
    import io.agora.rtc2.video.IVideoFrameObserver;
    import io.agora.rtc2.IAudioFrameObserver;
    import io.agora.base.VideoFrame;
    import java.nio.ByteBuffer;
    import android.widget.Button;
    ```

2.  **Define a variable to manage video processing**

    In `/app/java/com.example.<projectname>/MainActivity`, add the following declaration to the `MainActivity` class:

    ```java
    private boolean isZoomed = false;
    ```

### Implement processing of raw video and audio data

To register and use video and audio frame observers in your <Vpl k="CLIENT" />, take the following steps:

1.  **Set up the audio frame observer**

    `IAudioFrameObserver` gives you access to each audio frame after it is captured or access to each audio frame before it is played back. To setup the `IAudioFrameObserver`, add the following lines to the `MainActivity` class after variable declarations:

    ```java
    private final IAudioFrameObserver iAudioFrameObserver = new IAudioFrameObserver() {
        @Override
        public boolean onRecordAudioFrame(String channelId, int type, int samplesPerChannel,
                                          int bytesPerSample, int channels, int samplesPerSec, ByteBuffer buffer, long renderTimeMs, int avsync_type) {
            // Gets the captured audio frame.
            // Add code here to process the recorded audio.
            return false;
        }

        @Override
        public boolean onPlaybackAudioFrame(String channelId, int type, int samplesPerChannel,
                                            int bytesPerSample, int channels, int samplesPerSec, ByteBuffer buffer, long renderTimeMs, int avsync_type) {
            // Gets the audio frame for playback.
            // Add code here to process the playback audio.
            return false;
        }

        @Override
        public boolean onMixedAudioFrame(String channelId, int type, int samplesPerChannel,
                                         int bytesPerSample, int channels, int samplesPerSec, ByteBuffer buffer, long renderTimeMs, int avsync_type) {
            // Retrieves the mixed captured and playback audio frame.
            return false;
        }

        @Override
        public boolean onEarMonitoringAudioFrame(int type, int samplesPerChannel, int bytesPerSample, int channels, int samplesPerSec, ByteBuffer buffer, long renderTimeMs, int avsync_type) {
            return false;
        }

        @Override
        public boolean onPlaybackAudioFrameBeforeMixing(String channelId, int userId, int type, int samplesPerChannel,
                                                        int bytesPerSample, int channels, int samplesPerSec, ByteBuffer buffer, long renderTimeMs, int avsync_type) {
            // Retrieves the audio frame of a specified user before mixing.
            return false;
        }

        @Override
        public int getObservedAudioFramePosition() {
            return 0;
        }

        @Override
        public AudioParams getRecordAudioParams() {
            return null;
        }

        @Override
        public AudioParams getPlaybackAudioParams() {
            return null;
        }

        @Override
        public AudioParams getMixedAudioParams() {
            return null;
        }

        @Override
        public AudioParams getEarMonitoringAudioParams() {
            return null;
        }
    };
    ```

2.  **Set up the video frame observer**

    `IVideoFrameObserver` gives you access to each local video frame after it is captured and access to each remote video frame before it is played back. In this example, your modify the captured video frame buffer to crop and scale the frame and play a zoomed-in version of the video. To set up `IVideoFrameObserver`, add the following lines to the `MainActivity` class after the variable declarations:

    ```java
    private final IVideoFrameObserver iVideoFrameObserver = new IVideoFrameObserver() {
        @Override
        public boolean onCaptureVideoFrame(VideoFrame videoFrame) {
            if (isZoomed) {
                VideoFrame.Buffer buffer = videoFrame.getBuffer();
                int w = buffer.getWidth();
                int h = buffer.getHeight();
                int cropX = (w - 320)/2, cropY = (h - 240)/2, cropWidth = 320, cropHeight = 240, scaleWidth = 320, scaleHeight = 240;
                buffer = buffer.cropAndScale(cropX, cropY, cropWidth, cropHeight, scaleWidth, scaleHeight);
                videoFrame.replaceBuffer(buffer, 270, videoFrame.getTimestampNs());
            }
            return true;
        }

        @Override
        public boolean onPreEncodeVideoFrame(VideoFrame videoFrame) {
            return false;
        }

        @Override
        public boolean onScreenCaptureVideoFrame(VideoFrame videoFrame) {
            return false;
        }

        @Override
        public boolean onPreEncodeScreenVideoFrame(VideoFrame videoFrame) {
            return false;
        }

        @Override
        public boolean onMediaPlayerVideoFrame(VideoFrame videoFrame, int i) {
            return false;
        }

        @Override
        public boolean onRenderVideoFrame(String s, int i, VideoFrame videoFrame) {
            return false;
        }

        @Override
        public int getVideoFrameProcessMode() {
            // The process mode of the video frame. 0 means read-only, and 1 means read-and-write.
            return 1;
        }

        @Override
        public int getVideoFormatPreference() {
            return 1;
        }

        @Override
        public boolean getRotationApplied() {
            return false;
        }

        @Override
        public boolean getMirrorApplied() {
            return false;
        }

        @Override
        public int getObservedFramePosition() {
            return 0;
        }
    };
    ```

    Note that you must set the return value in `getVideoFrameProcessMode` to `1` in order for your raw data changes to take effect.

3.  **Register the video and audio frame observers**

    To receive callbacks declared in `IVideoFrameObserver` and `IAudioFrameObserver`, you must register the video and audio frame observers with the <Vg k="ENGINE" /> before joining a channel. To specify the format of audio frames captured by each `IAudioFrameObserver` callback, use the `setRecordingAudioFrameParameters`, `setMixedAudioFrameParameters` and `setPlaybackAudioFrameParameters` methods. To do this, add the following lines after `if (checkSelfPermission()) {` in the `joinChannel` method:

    ```java
    agoraEngine.registerVideoFrameObserver(iVideoFrameObserver);
    agoraEngine.registerAudioFrameObserver(iAudioFrameObserver);

    // Set the format of the captured raw audio data.
    int SAMPLE_RATE = 16000, SAMPLE_NUM_OF_CHANNEL = 1, SAMPLES_PER_CALL = 1024;

    agoraEngine.setRecordingAudioFrameParameters(SAMPLE_RATE, SAMPLE_NUM_OF_CHANNEL,
        Constants.RAW_AUDIO_FRAME_OP_MODE_READ_WRITE,SAMPLES_PER_CALL);
    agoraEngine.setPlaybackAudioFrameParameters(SAMPLE_RATE, SAMPLE_NUM_OF_CHANNEL,
        Constants.RAW_AUDIO_FRAME_OP_MODE_READ_WRITE,SAMPLES_PER_CALL);
    agoraEngine.setMixedAudioFrameParameters(SAMPLE_RATE, SAMPLE_NUM_OF_CHANNEL, SAMPLES_PER_CALL);
    ```

4.  **Unregister the video and audio observers when you leave a channel**

    When you leave a channel, you unregister the frame observers by calling the register frame observer method again with a `null` argument. To do this, add the following lines to the `leaveChannel(View view)` method before `agoraEngine.leaveChannel();`:

    ```java
    agoraEngine.registerVideoFrameObserver(null);
    agoraEngine.registerAudioFrameObserver(null);
    ```

5.  **Start and stop video processing**

    When a user presses the button, enable or disable video processing. To do this, add the following method to the `MainActivity` class:

    ```java
    public void setZoom (View view){
        isZoomed = !isZoomed;

        Button button = (Button) view;
        if (isZoomed)
            button.setText("Zoom Out");
        else
            button.setText("Zoom In");
    }
    ```

</PlatformWrapper>
<PlatformWrapper platform="web">

To modify raw audio frames before publishing, follow these steps:

1. Create a local audio track and publish it to the channel.
1. Use the Web Audio API to create an `AudioContext` and connect the audio track to an `AnalyserNode`.
1. Extract audio data using an `AnalyserNode` and process it. 

The following example extracts the audio data and logs it to the console.

```js
async function createAndProcessAudioTrack() {
  try {
    // Create a microphone audio track
    localAudioTrack = await AgoraRTC.createMicrophoneAudioTrack();
    
    // Publish the audio track to the RTC client
    await client.publish([localAudioTrack]);

    // Initialize the Web Audio API context for audio processing
    audioContext = new AudioContext();
    
    // Create a media stream source from the microphone track
    const source = audioContext.createMediaStreamSource(
      new MediaStream([localAudioTrack.getMediaStreamTrack()])
    );

    // Create an AnalyserNode to extract real-time audio frequency data
    analyser = audioContext.createAnalyser();
    analyser.fftSize = 2048; // Higher FFT size improves frequency resolution but increases processing load

    // Create a buffer to store the frequency data
    dataArray = new Uint8Array(analyser.frequencyBinCount);

    // Connect the audio source to the analyser
    source.connect(analyser);

    // Start continuously processing audio frames
    processAudioFrames();
  } catch (error) {
    console.error("Error creating or processing audio track:", error);
  }
}

function processAudioFrames() {
  if (!analyser) {
    console.warn("Analyser not initialized.");
    return;
  }

  // Retrieve frequency data from the analyser node
  analyser.getByteFrequencyData(dataArray);
  console.log("Audio Data:", dataArray); // Log real-time frequency data

  // Schedule the next frame for continuous audio analysis
  animationFrameId = requestAnimationFrame(processAudioFrames);
}
```

</PlatformWrapper>

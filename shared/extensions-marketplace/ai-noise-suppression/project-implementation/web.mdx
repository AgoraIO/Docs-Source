<PlatformWrapper platform="web">

1. Integrate <Vg k="RTEE_NS" /> into your app

    To install the  <Vg k="RTEE_NS" /> extension ([agora-extension-ai-denoiser](https://www.npmjs.com/package/agora-extension-ai-denoiser)), in the root of your project run the following command:

      `npm install agora-extension-ai-denoiser`


1. Import  <Vg k="RTEE_NS" />

    Add the following code to your `.js` file.

      ```javascript
      import {AIDenoiserExtension} from "agora-extension-ai-denoiser";
      ```

3. Dynamically load the Wasm  dependencies

    The <Vg k="RTEE_NS" /> extension depends on a few Wasm files. To ensure that the browser can load and execute these files, do the following:

    1. Publish the files located in the `node_modules/agora-extension-ai-denoiser/external` directory to the CDN or static resource server, and put them under one public path. In subsequent steps, you need to pass in the public path URL to create an` AIDenoiserExtension` instance. The extension then dynamically loads these files.

        Note:
            - If the host URL of the Wasm files is not the same as that of the web application, enable the CORS policy.
            - Do not put the Wasm files in an HTTP service, loading HTTP resources in the HTTPS domain is blocked by the browsers' security policy.

    2. If you have enabled the Content Security Policy (CSP), because Wasm files are not allowed to load in Chrome and Edge by default, you need to configure the CSP as follows:

        - For versions later than Chrome 97 and Edge 97 (Chrome 97 and Edge 97 included): Add `'wasm-unsafe-eval'` and `blob:` in the [`script-src`](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Content-Security-Policy/script-src) options. For example:
           ```xml
             <meta http-equiv="Content-Security-Policy" content="script-src 'self' 'wasm-unsafe-eval' blob:">
             ```

        - For versions earlier than Chrome 97 and Edge 97: Add `'unsafe-eval'` and `blob:` in the `script-src` options.

4. Register the <Vg k="RTEE_NS" /> extension

    Call <Link to= "{{global.API_REF_WEB_ROOT}}/interfaces/iagorartc.html#registerextensions">AgoraRTC.registerExtensions</Link>, and pass in the created `AIDenoiserExtension` instance. Optionally, listen for the callback reporting that the Wasm and JS files fail to load.

    ```typescript
    // Create an AIDenoiserExtension instance, and pass in the host URL of the Wasm files
    const denoiser = new AIDenoiserExtension({assetsPath:'./external'});
    // Check compatibility
    if (!denoiser.checkCompatibility()) {
     // The extension might not be supported in the current browser. You can stop executing further code logic
     console.error("Does not support AI Denoiser!");
    }
    // Register the extension
    AgoraRTC.registerExtensions([denoiser]);
    // (Optional) Listen for the callback reporting that the Wasm files fail to load
    denoiser.onloaderror = (e) => {
     // If the Wasm files fail to load, you can disable the extension, for example:
     // openDenoiserButton.enabled = false;
     console.log(e);
    }
    ```
    Best practice is to create one <code>AIDenoiserExtension</code> instance only.

5. Create an `IAIDenoiserProcessor` instance

    Call the `createProcessor` method to create a `processor`, and set whether to enable the extension by default. If you do not set the default state, the extension inherits the previous state.

    ```typescript
    // Create a processor
    const processor = denoiser.createProcessor();
    // Enable the extension by default
    processor.enable();
    // Disable the extension by default
    // processor.disable();
    ```

6. Inject the extension to the audio processing pipeline

    Call <Link to="{{global.API_REF_WEB_ROOT}}/interfaces/imicrophoneaudiotrack.html#pipe">pipe</Link> and specify the <Link to="{{global.API_REF_WEB_ROOT}}/interfaces/imicrophoneaudiotrack.html#processordestination">processorDestination</Link> property.

    ```typescript
    // Create a local video track
    const audioTrack = await AgoraRTC.createMicrophoneAudioTrack();
    // Inject the extension to the audio processing pipeline
    audioTrack.pipe(processor).pipe(audioTrack.processorDestination);
    await processor.enable();
    ```

7. Enable or disable the extension

    Call the `enable` or `disable` methods as needed.

      ```typescript
      () => {
        if (processor.enabled) {
          await processor.disable();
        } else {
          await processor.enable();
        }
      }
      ```
8. Set the noise suppression mode and level
   ```javascript
   // Listen for the callback reporting that the noise suppression process takes too long
   processor.onoverload = async (elapsedTime) => {
     console.log("overload!!!");
     // Switch from AI noise suppression to stationary noise suppression
     await processor.setMode("STATIONARY_NS");
     // Disable AI noise suppression
     await processor.disable();
   }
   ```

9. Set up logging

    Best practice for logging is to dump audio data. This significantly improves the efficiency of troubleshooting. To do this, call the `dump` method, and listen for the `ondump` and `ondumpend` callbacks.

   ```typescript
   processor.ondump = (blob, name) => {
     // Dump the audio data to a local folder in PCM format
     const objectURL = URL.createObjectURL(blob);
     const tag = document.createElement("a");
     tag.download = name;
     tag.href = objectURL;
     tag.click();
     setTimeout(() => {URL.revokeObjectURL(objectURL);}, 0);
   }

   processor.ondumpend = () => {
     console.log("dump ended!!");
   }

   processor.dump();
   ```


</PlatformWrapper>

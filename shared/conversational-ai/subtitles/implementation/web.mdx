import CodeBlock from '@theme/CodeBlock';

1. **Integrate the toolkit**

    Copy the [`conversational-ai-api`](https://github.com/AgoraIO-Community/Conversational-AI-Demo/tree/main/Web/Scenes/VoiceAgent/src/conversational-ai-api) file to your project and import the toolkit before calling its API. Refer to [Folder structure](#folder-structure) to understand the role of each file.

1. **Implement subtitle UI rendering logic**

    The subtitle UI toolkit receives transcription data from the client component and displays the messages using a simple React component. The `TRANSCRIPTION_UPDATED` event callback always returns the entire list of subtitle messages. Only the latest messages may be updated, but your UI should re-render based on the full list provided in each callback.

    <Admonition type="info" info="info">
    For better state sharing across components, consider using a state management library such as `zustand` or redux instead of `React.useState`.
    </Admonition>

    <CodeBlock language="js" showLineNumbers>
    {`import * as React from "react"
    import {
    type IUserTranscription,
    type IAgentTranscription,
    type ISubtitleHelperItem,
    EConversationalAIAPIEvents,
    } from "@/conversational-ai-api/type"
    import { ConversationalAIAPI } from "@/conversational-ai-api"

    // Listen for transcription updates to display subtitles in real time
    export const ChatHistory = () => {
    const [chatHistory, setChatHistory] = React.useState<
        ISubtitleHelperItem<Partial<IUserTranscription | IAgentTranscription>>[]
    >([])

    return (
        <>
        {chatHistory.map((message) => (
            <div key={\`\$\{message.uid\}-\$\{message.turn_id\}\`}>
            {message.uid}: {message.text}
            </div>
        ))}
        </>
    )}`}
    </CodeBlock>

1. **Create a subtitle processing toolkit instance**

    Before joining a <Vg k="VSDK"/> channel, create video and <Vg k="SIG" /> engine instances and pass in to the toolkit instance.

    <CodeBlock language="js" showLineNumbers>
    {`// Initialize the component
    ConversationalAIAPI.init({
      rtcEngine,
      rtmEngine,

      /**
      * Set the rendering mode for transcription subtitles. Available options:
      * - ESubtitleHelperMode.WORD: Word-by-word rendering mode. The subtitle content received from the callback 
      *   is rendered to the UI one word at a time.
      * - ESubtitleHelperMode.TEXT: Sentence-by-sentence rendering mode. The full subtitle content from the callback 
      *   is rendered to the UI at once.
      *
      * If not specified, the mode is determined automatically based on the message, or it can be set manually.
      */
      renderMode: ESubtitleHelperMode.WORD, 
    })

    // Get the API instance (singleton)
    const conversationalAIAPI = ConversationalAIAPI.getInstance()
    conversationalAIAPI.on(
      EConversationalAIAPIEvents.TRANSCRIPTION_UPDATED,
      setChatHistory
    )`}
    </CodeBlock>

1. **Subscribe to the channel**
    Agent-related events are delivered through <Vg k="SIG" /> messages. Before starting an agent session, call `subscribeMessage` to receive these events:

    <CodeBlock language="ts" showLineNumbers>
    {`conversationalAIAPI.subscribeMessage(channel_name)`}
    </CodeBlock>


1. **Agent joins the channel**

    To create a conversational agent interface, make a `POST` request and configure the following parameters:

    | Parameter                         | Required | Description                                    |
    | --------------------------------- | -------- | ---------------------------------------------- |
    | `advanced_features.enable_rtm`    | Yes      | Enables the <Vg k="SIG" /> service. |
    | `parameters.data_channel`         | Yes      | Sets the data transmission channel to `"rtm"`. |
    | `parameters.enable_metrics`       | Optional | Enables agent performance metrics.             |
    | `parameters.enable_error_message` | Optional | Enables agent error event reporting.           |

    After a successful request, the agent joins the specified <Vg k="VSDK"/> channel and is ready to interact with the user.

1. **Receive subtitles**

    Call the `addHandler` method to register and implement the subtitle transcription callback:

    <CodeBlock language="ts" showLineNumbers>
    {`// Listen for transcription updates to display subtitles in real time
    conversationalAIAPI.addHandler({
    onSubtitleMessage: (subtitle) => {
        // Handle subtitle content
        console.log('Subtitle received:', subtitle)
        renderSubtitlesToDOM(subtitle)
    },
    })`}
    </CodeBlock>

1. **Unsubscribe from the channel**

    After each agent session ends, unsubscribe from channel messages to release resources associated with callback events:

    <CodeBlock language="tsx" showLineNumbers>
    {`conversationalAIAPI.unsubscribeMessage(channel_name)`}
    </CodeBlock>

1. **Release resources**

    At the end of each call, use the `destroy` method to clean up the cache.

    <CodeBlock language="ts" showLineNumbers>
    {`conversationalAIAPI.destroy()`}
    </CodeBlock>


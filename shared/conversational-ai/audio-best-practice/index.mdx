
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import AndroidImplementation from '@docs/shared/conversational-ai/transcripts/implementation/android.mdx';
import SwiftImplementation from '@docs/shared/conversational-ai/transcripts/implementation/swift.mdx';
import WebImplementation from '@docs/shared/conversational-ai/transcripts/implementation/web.mdx';
import AndroidReference from '@docs/shared/conversational-ai/transcripts/reference/android.mdx';
import SwiftReference from '@docs/shared/conversational-ai/transcripts/reference/swift.mdx';
import WebReference from '@docs/shared/conversational-ai/transcripts/reference/web.mdx';

When interacting with conversational AI in real time, you can enable real-time transcripts to display the conversation content. This page explains how to implement real-time transcripts in your app.

## Understand the tech

Agora provides a flexible, scalable, and standardized conversational AI engine toolkit. The toolkit supports **iOS**, **Android**, and **Web** platforms, and encapsulates scenario-based APIs. You can use these APIs to integrate the capabilities of the Agora Chat SDK and Video SDK to enable the following features:

* [`Interrupt agents`](/conversational-ai/develop/interrupt-agent)
* [`Display live transcripts`](/conversational-ai/develop/transcripts)
* [`Receive event notifications`](/conversational-ai/develop/event-notifications)
* [`Set optimal audio parameters`](/conversational-ai/develop/audio-output) (iOS and Android only)

The toolkit retrieves transcript content through the `onTranscriptUpdated` callback. It supports monitoring different types of transcript data, including:

* **Agent transcript**: Transcript of the agent’s speech, including real-time updates and final results.
* **User transcript**: Transcript of the user’s speech, with support for real-time display and status tracking.
* **Transcription status**: Indicates whether transcription is *in progress*, *completed*, or *interrupted*.

The following diagram shows the workflow for implementing live transcripts:

<details>
<summary>Transcript module workflow</summary>

![](/images/conversational-ai/transcript-module.svg)
</details>

## Prerequisites

Before you begin, make sure you have implemented the [Quickstart](#) in your project.

## Implementation

This section describes how to receive transcript content from the transcript processing module and display it on your app UI.

<Tabs groupId="platform">
<TabItem value="android" label="Android" default>
<AndroidImplementation />
</TabItem>

<TabItem value="swift" label="iOS" default>
<SwiftImplementation />
</TabItem>

<TabItem value="web" label="Web" default>
<WebImplementation />
</TabItem>
</Tabs>




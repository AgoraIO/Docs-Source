---
title: 'Core concepts'
sidebar_position: 1
type: docs
platform_selector: false
description: >
  Understand the foundational concepts behind Conversational AI Engine.
---

Learn the core concepts behind <Vpd k="NAME" />. This guide covers two areas: 
Conversational AI concepts explain how agents work and which AI workflow to choose, while RTC concepts cover the real-time communication infrastructure that powers voice interactions.

## Conversational AI concepts

These concepts explain how <Vpd k="NAME" /> orchestrates real-time conversations between users and AI agents. Understanding these fundamentals will help you design better applications and make informed technical decisions.

### What is Conversational AI Engine?

<Vpd k="NAME" /> is a cloud-based orchestration that enables real-time voice, video, and avatar interactions between users and AI agents within <Vg k="COMPANY" /> channels. It provides the infrastructure to build agents that can hear, think, and respond naturally. The engine handles the complex coordination between speech recognition, language models, and speech synthesis so you can focus on creating intelligent conversational experiences.

The engine supports two distinct AI workflows:
- **Cascading Pipeline (ASR-LLM-TTS)**: Chains specialized AI services for full control over each component
- **Multimodal Workflow (Voice-to-Voice)**: Uses a single AI model for faster, more natural conversations

You bring the AI models and conversational logic. The engine handles real-time audio transport, session management, and orchestration at scale.

### Why use Conversational AI Engine?

<Vpd k="NAME" /> differentiates itself through several key advantages:

- **Ultra-low latency**: End-to-end response times as low as 650ms make conversations feel natural. The engine's optimized pipeline minimizes overhead between speech recognition, LLM processing, and speech synthesis.

- **Bring your own models**: Mix and match from 15+ providers for ASR, LLM, TTS, and MLLM. Use <Vg k="COMPANY" />'s built-in ARES ASR, OpenAI, Anthropic, Google Gemini, Azure, Deepgram, ElevenLabs, and more, or connect your own custom endpoints.

- **Choose your workflow**: Start with the cascading pipeline for full control and easy debugging, then switch to multimodal voice-to-voice when you need the fastest response times. Both workflows use the same API.

- **Network resilience at scale**: AI-optimized transmission maintains stable conversations with up to 80% packet loss. Handles brief network disconnects (3-5s) without dropping context or interrupting the conversation.

- **Focus on logic, not infrastructure**: You define what your agent should say and how it should behave. The engine handles audio transport, session management, token generation, and scaling, so you can spend your time on the conversational experience, not WebRTC debugging.

### What can you build?

<Vpd k="NAME" /> enables a wide range of conversational AI applications:

- **AI customer support**: Deploy voice agents that handle customer inquiries through web, mobile, or phone (PSTN). Route complex issues to human agents with full conversation context.

- **Virtual assistants with avatars**: Combine conversational AI with visual avatars from third-party providers for engaging multimodal experiences in apps, kiosks, or virtual environments.

- **Healthcare telehealth**: Build HIPAA-compliant AI triage agents that assess symptoms, schedule appointments, and provide health information before connecting patients with providers.

- **Educational tutors**: Create AI tutors that explain concepts, answer questions, and adapt to student learning pace across subjects and languages.

- **Interactive voice response (IVR)**: Replace traditional phone menus with natural language interfaces. Users speak naturally instead of navigating numbered options.

- **Smart home devices**: Integrate voice AI into IoT devices for natural language control of home automation, information lookup, and interactive assistance.

The engine provides the real-time foundation—low latency, scalable performance, and flexible tools—needed to run production-ready AI experiences across any platform.

### How does Conversational AI Engine work?

Understanding the architecture helps you design better applications and debug issues effectively.

#### Two-component architecture

Building a conversational AI application requires two core components:

1. **Client App (Frontend)**

    The user-facing application where end-users interact with AI agents through voice, video, or text. This can be a:
    - Web application (React, Vue, vanilla JavaScript)
    - Native mobile app (iOS, Android)
    - PSTN/SIP telephony connection
    - IoT or embedded device

    Users join channels using the <Vg k="COMPANY" /> SDK. The client handles audio capture from the user's microphone, sends it to the channel, receives audio from the agent, and plays it through speakers.

2. **Agent Manager (Backend Server)**

    Your backend server that manages the agent lifecycle. It:
    - Authenticates with <Vg k="COMPANY" />'s RESTful API using Customer ID and Customer Secret
    - Generates authentication tokens for agents to join channels
    - Deploys agents into channels by calling the `/join` API
    - Defines the AI workflow (which models to use, system prompts, voices)
    - Handles agent configuration (interruption behavior, idle timeouts)
    - Stops agents when conversations end

**Why separate?**

Security. Your backend holds sensitive credentials that must never be exposed to client applications:
- **Customer ID and Customer Secret**: Full API access to your <Vg k="COMPANY" /> project
- **App Certificate**: Used to generate authentication tokens

The backend acts as a trusted intermediary that controls who can create agents and when. Clients never have direct access to agent management APIs. They request agents through your authenticated backend, which enforces your business logic and access control.

![Architecture](/images/conversational-ai/convo-ai-architecture.svg)

{/*
[IMAGE: Architecture diagram
- Left box: Client Apps (mobile, web, phone icons) labeled "End Users"
- Center: Agora Cloud (SD-RTN network) with "Channels" 
- Right box: Your Backend Server labeled "Agent Manager"
- Arrows: Users → Join Channel, Backend → Deploy Agent via API, Agent ↔ Users in channel (bidirectional audio)]
*/}

### Cascading and multimodal workflows

<Vpd k="NAME" /> supports two distinct workflows for processing conversations:

- **Cascading pipeline (ASR-LLM-TTS)**

    Chains three specialized AI services: 

    1. **ASR (Automatic Speech Recognition)**: Converts user's voice input into text
    2. **LLM (Large Language Model)**: Processes the text and generates an intelligent text response
    3. **TTS (Text-to-Speech)**: Converts the LLM's text response back into natural-sounding speech

    This workflow gives you full control over each component and the flexibility to mix providers. For example, you might use <Vg k="COMPANY" />'s ARES for fast speech recognition, OpenAI's GPT-4o for smart responses, and Azure's neural voices for natural speech output. 

- **Multimodal workflow (Voice-to-Voice)**

    Uses a single AI model such as OpenAI Realtime API or Google Gemini Live that processes voice input directly and generates voice output. This eliminates the need for separate ASR or TTS, cutting latency in half.


![Comparison of cascading pipeline and multimodal workflows](/images/conversational-ai/convo-ai-cascade.svg)

Each workflow has distinct trade-offs:

| Factor | Cascading pipeline | Multimodal workflow |
|--------|-------------------|---------------------|
| **Provider flexibility** | Switch providers independently (for example, different TTS voices by language) | Single provider handles everything |
| **Observability** | Log and analyze text transcripts at each step | Audio in, audio out (limited visibility) |
| **Advanced features** | Function calling and tool use supported | Support varies by provider |
| **Cost control** | Mix free/paid services to optimize costs | Fixed model pricing |
| **Debugging** | Can debug each step separately | End-to-end debugging only |
| **Latency** | Latency from ASR, LLM, and TTS adds up | Direct voice-to-voice (lowest latency) |
| **Conversation quality** | Tone and emotion nuance lost when converting to text | Preserves tone and emotion natively |
| **Configuration** | Configure three components | Configure one model |

Choose the cascading pipeline for production apps that need flexibility, observability, and cost control. Choose the multimodal workflow when conversation speed and emotional nuance matter most.

<Admonition type="info">
When you enable MLLM mode (`enable_mllm: true`), separate ASR, LLM, and TTS configurations are ignored—the MLLM handles everything.
</Admonition>

### Agent lifecycle

Agents go through a lifecycle from creation to cleanup:

1. **Start**: Your backend calls <Vg k="COMPANY" />'s REST API (`/join`) with agent configuration. The API returns immediately with an `agent_id`.

2. **Joining**: <Vg k="COMPANY" /> deploys an agent instance in the cloud. The agent authenticates with its RTC token and joins the specified channel as a participant.

3. **Running**: Agent is now active in the channel. It listens to audio from specified user UIDs, processes audio through the AI pipeline (ASR → LLM → TTS or MLLM), and speaks responses back into the channel.

4. **Idle**: If no audio is detected for the `idle_timeout` period (configurable), the agent remains in the channel but may eventually time out and leave.

5. **Stop**: Your backend calls the REST API (`/leave`) to stop the agent, or the agent times out. The agent leaves the channel and resources are cleaned up.

**Timeline:** The entire start-to-running process takes 1-3 seconds. Users typically hear the agent's greeting message within 2-3 seconds of the agent starting.

**Important**: Store the `agent_id` returned from the start API—you'll need it to stop, update, or query the agent's status.

### Remote RTC UIDs

The `remote_rtc_uids` parameter specifies which user the agent should listen to. The agent **only** processes audio from this user. If a user's UID is not included, the agent will not hear them.


**Why this matters:**
- **Privacy**: Ensures the agent only listens to the intended user
- **Cost control**: Agent only processes audio from the intended participant
- **Multi-party scenarios**: You can have multiple users in a channel but only let the agent interact with a specific user

The parameter accepts an array, but only a single remote UID is currently supported.

**Sample configuration:**

```json
{
  "remote_rtc_uids": ["1002"]
}
```

{/*
Multiple users (group conversation):
```json
{
  "remote_rtc_uids": ["1002", "1003", "1004"]
}
```
*/}

**Common mistake**: Forgetting to add a user's UID to this list, then wondering why the agent doesn't respond to them.

## RTC concepts

<Vpd k="NAME" /> is built on <Vg k="COMPANY" />'s real-time communication infrastructure. Understanding the following RTC fundamentals will help you secure your application, control who can talk to your agents, and debug audio issues effectively.

### Understanding Real-Time Communication (RTC)

Real-Time Communication (RTC) enables near-instant audio and video exchange between participants. Unlike HTTP (designed for request-response web browsing), RTC protocols are optimized for continuous media streams with minimal latency—critical for natural conversations where delays break the flow.

WebRTC is the browser-native technology that powers real-time media on the web. All major browsers support it, making it the standard for voice and video applications. But WebRTC is complex to operate at scale; handling NAT traversal, codec negotiation, network adaptation, and quality management requires significant infrastructure.

<Vg k="COMPANY" />'s SD-RTN™ (Software Defined Real-Time Network) handles this operational complexity. It routes audio and video with ultra-low latency, automatically adapts to changing network conditions, and maintains stable connections even with packet loss. When you use <Vpd k="NAME" />, your AI agents join channels through the same battle-tested infrastructure that powers millions of real-time sessions daily.

### Agora channel

A channel is a virtual room identified by a name. For example, `"support-room-123"` or `"user-456-session"`. Both end-users and AI agents join channels to exchange real-time audio, video, and data. Think of it like a video conference room. Everyone who joins the same channel can communicate with each other.

Channels are created automatically when the first participant joins and exist as long as participants remain connected. You control who can join by generating authentication tokens scoped to specific channel names.

In a typical conversational AI scenario:
1. Your client app joins a channel using the Agora SDK
2. Your backend deploys an AI agent into the same channel
3. The agent and user exchange audio in real-time through the channel
4. When the conversation ends, both leave the channel

### Authentication token

An authentication token is a temporary credential that grants permission to access <Vg k="COMPANY" /> services. A single token can authenticate both:

- **RTC (Real-Time Communication)**: Join channels for audio/video communication
- **RTM (Signaling)**: Receive real-time transcriptions and custom information from agents

**Key characteristics:**
- Generated server-side using your App ID and App Certificate
- Scoped to a specific channel name and user UID
- Expire after a set time (typically 1-24 hours)
- Can include privileges for RTC, RTM, or both services

Your backend generates tokens for both human users and AI agents. Never expose your App Certificate to client applications. Tokens should always be generated server-side and delivered to clients through your authenticated API.

**Why RTM matters for Conversational AI:**

When you enable RTM (`enable_rtm: true` in the agent configuration), the agent can send real-time data through Signaling channels:
- **Live transcriptions**: Display what the user and agent are saying in real-time. See [Display live transcripts](/conversational-ai/develop/transcripts).
- **Custom information**: Deliver structured data from your agent to the client. See [Send custom information](/conversational-ai/develop/custom-information).

The agent reuses the same token for both RTC (audio) and RTM (data) services.

**Security model:**

```
Client requests to join channel "room-123"
         ↓
Your backend verifies user is authorized
         ↓
Backend generates token for channel "room-123" and user's UID
(with RTC + RTM privileges if transcriptions are needed)
         ↓
Client uses token to join channel through Agora SDK
Agent reuses the same token for both RTC and RTM
```

### User ID (UID)

A unique identifier for each participant in a channel. UIDs can be integers (`0` to `2^32-1`) or strings (up to 255 characters). Users have UIDs, and your agent has a UID too.

**Why UIDs matter:**
- Clients use UIDs to identify who's speaking
- You specify which UIDs the agent should listen to
- UIDs must be unique within a channel

**Example in a customer support scenario:**
- Customer UID: `1002`
- Support agent (AI) UID: `1001`
- Supervisor (human, optional) UID: `1003`

You configure your agent to listen to a specific UID through the `remote_rtc_uids` parameter. The agent only processes audio from the specified UID giving you control over who can interact with the agent.


## How can you learn more?

This documentation site is organized to guide you from concepts to implementation to production:

**Next steps:**
- **[Agent quickstart](/conversational-ai/get-started/quickstart)**: Build your first backend server that manages agents
- **[Product overview](/conversational-ai/overview/product-overview)**: Explore detailed feature descriptions and capabilities

**Advanced features:**
- **[Custom LLM integration](/conversational-ai/develop/custom-llm)**: Add RAG for knowledge retrieval, function calling, and multi-agent flows
- **[Interrupt agent](/conversational-ai/develop/interrupt-agent)**: Configure how users can interrupt the agent
- **[Webhooks](/conversational-ai/develop/webhooks)**: Monitor agent events and performance in real-time

**Production preparation:**
- **[Pricing](/conversational-ai/overview/pricing)**: Understand costs and optimize your implementation
- **[REST API reference](/conversational-ai/rest-api/agent/join)**: Complete API documentation for agent management

**AI model selection:**
- **[ASR providers](/conversational-ai/models/asr/overview)**: Speech recognition options
- **[LLM providers](/conversational-ai/models/llm/overview)**: Language model options
- **[TTS providers](/conversational-ai/models/tts/overview)**: Text-to-speech options
- **[MLLM providers](/conversational-ai/models/mllm/overview)**: Multimodal voice-to-voice options

Start with the quickstart guide to get hands-on experience, then explore advanced features as your needs evolve.


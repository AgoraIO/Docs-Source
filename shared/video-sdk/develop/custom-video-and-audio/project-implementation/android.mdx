<PlatformWrapper platform="android">

### Implement a custom video source

In this section you create the basic framework required to push video frames from a custom source. Depending on the type of your source, you add your own code to this framework that converts the source data to `VideoFrame` data. To create the basic framework, take the following steps:

1. **Import the required <Vg k="COMPANY" /> and Android libraries**

    You use the Android `TextureView` and `SurfaceTexture` objects for rendering custom video. The video data from the `SurfaceTexture` is converted to a `VideoFrame` before it is pushed to the channel. To use these libraries in your <Vpl k="CLIENT" />, add the following statements after the last `import` statement in `/app/java/com.example.<projectname>/MainActivity`.

    ```java
    import io.agora.base.VideoFrame;
    import android.graphics.SurfaceTexture;
    import android.view.TextureView;
    import androidx.annotation.NonNull;
    ```

1. **Define variables to process and push video data**

    In `/app/java/com.example.<ProjectName>/MainActivity`, add the following declarations to the `MainActivity` class:

    ```java
    private TextureView previewTextureView;
    private SurfaceTexture previewSurfaceTexture;

    private boolean mTextureDestroyed = false;
    private boolean mPreviewing = false;
    ```

1. **Enable custom video track publishing**

    When a user presses **Join**, you configure `ChannelMediaOptions` to enable publishing of the captured video from a custom source. You set the external video source, and set up a `TextureView` for the custom video preview. To do this: 

    1. Add the following lines to the `joinChannel(View view)` method in the `MainActivity` class after `ChannelMediaOptions options = new ChannelMediaOptions();`:

        ```java
        // Enable publishing of the captured video from a custom source
        options.publishCustomVideoTrack = true;
        // Configure the external video source.
        agoraEngine.setExternalVideoSource(true, true, Constants.ExternalVideoSourceType.VIDEO_FRAME);
        // Check whether texture encoding is supported
        showMessage(agoraEngine.isTextureEncodeSupported() ? "Texture encoding is supported" : 
                "Texture encoding is not supported");
                // Set up a preview TextureView for the custom video
        setupCustomLocalVideoPreview();
        // Show the preview TextureView
        previewTextureView.setVisibility(View.VISIBLE);
        ```

    1. In the `joinChannel(View view)` method, *remove* the following lines:

        ```java
        setupLocalVideo();
        localSurfaceView.setVisibility(View.VISIBLE);
        ```

1. **Set up a TextureView for the custom video**

    Create a new `TextureView` object, and add a `SurfaceTextureListener` to it. The listener triggers the `onSurfaceTextureAvailable` callback when a `SurfaceTexture` becomes available. You add the `TextureView` to the `FrameLayout` container to display it in the UI. To do this, add the following method to the `MainActivity` class:

    ```java
    private void setupCustomLocalVideoPreview() {
        // Create TextureView
        previewTextureView = new TextureView(getBaseContext());
        // Add a SurfaceTextureListener
        previewTextureView.setSurfaceTextureListener(surfaceTextureListener);
        // Add the TextureView to the local video FrameLayout
        FrameLayout container = findViewById(R.id.local_video_view_container);
        container.addView(previewTextureView,320,240);
    }
    ```

1. **Define the `SurfaceTextureListener`**

    When a `SurfaceTexture` becomes available, you create a `previewSurfaceTexture` and set its `onFrameAvailableListener` listener. You set up and configure your custom video source, set its `SurfaceTexture` to the `previewSurfaceTexture`, and start the preview. To do this, add the following definition of the `surfaceTextureListener` to the `MainActivity` class:

    ```java
    private final TextureView.SurfaceTextureListener surfaceTextureListener = new TextureView.SurfaceTextureListener(){
        @Override
        public void onSurfaceTextureAvailable(@NonNull SurfaceTexture surface, int width, int height) {
            // Invoked when a TextureView's SurfaceTexture is ready for use.

            if (mPreviewing) {
                // Already previewing custom video
                return;
            }

            showMessage("Surface Texture Available");
            mTextureDestroyed = false;

            // Set up previewSurfaceTexture
            previewSurfaceTexture = new SurfaceTexture(true);
            previewSurfaceTexture.setOnFrameAvailableListener(onFrameAvailableListener);

            // Add code here to set up and configure the custom video source 
            // Add code here to set SurfaceTexture of the custom video source to previewSurfaceTexture
            
            // Start preview
            mPreviewing = true;
        }

        @Override
        public void onSurfaceTextureSizeChanged(@NonNull SurfaceTexture surface, int width, int height) {
        }

        @Override
        public boolean onSurfaceTextureDestroyed(@NonNull SurfaceTexture surface) {
            mTextureDestroyed = true;
            return false;
        }

        @Override
        public void onSurfaceTextureUpdated(@NonNull SurfaceTexture surface) {

        }
    };
    ```

1. **Push the video frames**

    The `onFrameAvailableListener` callback is triggered when a new video frame is available. In the callback, you convert the `SurfaceTexture` data to a <Vg k="VSDK" /> `VideoFrame` and push the frame to the channel.
    To do this, add the following `OnFrameAvailableListener` to the `MainActivity` class:

    ```java
    private final SurfaceTexture.OnFrameAvailableListener onFrameAvailableListener = new SurfaceTexture.OnFrameAvailableListener() {
        @Override
        public void onFrameAvailable(SurfaceTexture surfaceTexture) {
            // Callback to notify that a new stream video frame is available.
            
            if (isJoined) {
                // Configure the external video frames and send them to the SDK
                VideoFrame videoFrame = null;

                // Add code here to convert the surfaceTexture data to a VideoFrame object

                // Send VideoFrame to the SDK
                agoraEngine.pushExternalVideoFrame(videoFrame);
            }
        }
    };
    ```

### Implement a custom audio source

To push audio from a custom source to a channel, take the following steps:

1. **Import the required Android and Java libraries**

    You use an `InputStream` to read the contents of the custom audio source. The <Vpl k="CLIENT" /> starts a separate `Process` to read and push the audio data. To use these libraries in your <Vpl k="CLIENT" />, add the following statements after the last `import` statement in `/app/java/com.example.<projectname>/MainActivity`.

    ```java
    import android.os.Process;
    import java.io.InputStream;
    import java.io.IOException;
    ```

1. **Define variables to manage and push the audio stream**

    In `/app/java/com.example.<projectname>/MainActivity`, add the following declarations to the `MainActivity` class:

    ```java
    // Audio file parameters
    private static final String AUDIO_FILE = "applause.wav"; // raw audio file
    private static final Integer SAMPLE_RATE = 44100;
    private static final Integer SAMPLE_NUM_OF_CHANNEL = 2;
    private static final Integer BITS_PER_SAMPLE = 16;
    private static final Integer SAMPLES = 441;
    private static final Integer BUFFER_SIZE = SAMPLES * BITS_PER_SAMPLE / 8 * SAMPLE_NUM_OF_CHANNEL;
    private static final Integer PUSH_INTERVAL = SAMPLES * 1000 / SAMPLE_RATE;

    private InputStream inputStream;
    private Thread pushingTask = new Thread(new PushingTask());
    private boolean pushing = false;
    ```

1. **Add a raw audio file to the project**

    In this example, you use an audio file as the source of your custom audio data. To add the audio file to your Android project, create a folder `app\src\main\assets` and add a [sample audio file](\files\applause.wav) in `*.wav` or `*.raw` format to this folder. Update the value of the `AUDIO_FILE` variable to show the audio file name. Also make sure that the values of the audio file parameters in your code match the audio file you placed in the assets folder.

1. **Enable custom audio track publishing**

    When a user presses **Join**, you set the `ChannelMediaOptions` to disable the microphone audio track and enable the custom audio track. You also enable custom audio local playback and set the external audio source. To do this, add the following lines to the `joinChannel(View view)` method in the `MainActivity` class after `options.clientRoleType = Constants.CLIENT_ROLE_BROADCASTER;`:

    ```java
    options.publishMicrophoneTrack = false; // Disable publishing microphone-captured audio
    options.publishCustomAudioTrack = true; // Enable publishing custom audio
    options.enableAudioRecordingOrPlayout = true;

    agoraEngine.enableCustomAudioLocalPlayback(0, true);
    agoraEngine.setExternalAudioSource(true, SAMPLE_RATE, SAMPLE_NUM_OF_CHANNEL, 2, false, true);
    ```

1. **Open the audio file**

    When the <Vpl k="CLIENT" /> starts, you open the audio file. To do this, add the following lines at the bottom of the `onCreate` method:

    ```java
    try {
        inputStream = this.getResources().getAssets().open(AUDIO_FILE);
    } catch (IOException e) {
        e.printStackTrace();
    }
    ```

1. **Start the task to push audio frames**

    When a user successfully joins a channel, you start the task that pushes audio frames. To do this, add the following lines at the bottom of the `onJoinChannelSuccess` callback in the `MainActivity` class:

    ```java
    pushing = true;
    pushingTask.start();
    ```

1. **Read the input stream into a buffer**

    You read data from the input stream into a buffer. To do this, add the following method to the `MainActivity` class:

    ```java
    private byte[] readBuffer() {
        int byteSize = BUFFER_SIZE;
        byte[] buffer = new byte[byteSize];
        try {
            if (inputStream.read(buffer) < 0) {
                inputStream.reset();
                return readBuffer();
            }
        } catch (IOException e) {
            e.printStackTrace();
        }
        return buffer;
    }
    ```

1. **Push the audio frames**

    You push the data in the buffer as an audio frame using a separate process. To do this, define the following `Runnable` class in the `MainActivity` class:

    ```java
    class PushingTask implements Runnable {
        long number = 0;

        @Override
        public void run() {
            Process.setThreadPriority(Process.THREAD_PRIORITY_URGENT_AUDIO);
            while (pushing) {
                long before = System.currentTimeMillis();
                agoraEngine.pushExternalAudioFrame(readBuffer(), 0);
                long now = System.currentTimeMillis();
                long consuming = now - before;
                if(consuming < PUSH_INTERVAL){
                    try {
                        Thread.sleep(PUSH_INTERVAL - consuming);
                    } catch (InterruptedException e) {
                    }
                }
            }
        }
    }
    ```

1. **Close the audio file**

    When the <Vpl k="CLIENT" /> is closed, you close the audio file. To do this, add the following lines at the bottom of the `onDestroy` method:

    ```java
    try {
        inputStream.close();
    } catch (IOException e) {
        e.printStackTrace();
    }
    ```

</PlatformWrapper>

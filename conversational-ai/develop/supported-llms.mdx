---
title: Supported LLMs
sidebar_position: 0.5
platform_selector: false
description: >
  Integrate a custom LLM into Agora's Conversational AI Engine.
---

Large Language Models (LLMs) power the conversational intelligence in your AI agents. They process user input, invoke tools, and generate responses. Agora supports multiple LLM providers, allowing you to choose the best model for your specific requirements.

## Understand the tech

To integrate the LLM of your choice, follow these steps:

1. Choose your LLM provider from the [Supported LLMs](#supported-llms) table
1. Obtain an API key from the provider's console
1. Copy the sample configuration for your chosen provider
1. Replace `<api_key>` with your actual API key
1. Customize the `system_messages` for your use case
1. Specify the configuration in the request body as `properties` > `llm` when [Starting a conversational AI agent](../rest-api/join)


## Supported LLMs

<Vpd k="NAME" /> currently supports the following LLMs:

| Provider         | Style       | Documentation     |
|------------------|-------------|-------------------|
| OpenAI           | `openai`      | https://platform.openai.com/docs/api-reference/responses/create    |
| Azure OpenAI     | `openai`      | https://learn.microsoft.com/en-us/azure/ai-services/openai/reference              |
| Google Gemini    | `gemini`      | https://cloud.google.com/docs/authentication/rest   |
| Google Vertex AI | `gemini`      | https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/system-instructions |
| Claude           | `anthropic`   | https://docs.anthropic.com/en/api/messages          |
| Dify             | `dify`        | [Agora Conversational AI - Dify Marketplace](https://marketplace.dify.ai/plugins/plutoless/convoai)          |
| Custom LLM       | `openai`      | https://docs.agora.io/en/conversational-ai/develop/custom-llm      |

### OpenAI

OpenAI provides state-of-the-art language models optimized for different use cases.

#### Sample configuration

The following example shows a starting `llm` parameter configuration you can use when you [Start a conversational AI agent](../rest-api/join).

```json
"llm": {
    "url": "https://api.openai.com/v1/chat/completions",
    "api_key": "<your_llm_key>",
    "system_messages": [
      {
        "role": "system",
        "content": "You are a helpful chatbot."
      }
    ],
    "max_history": 32,
    "greeting_message": "Hello, how can I assist you",
    "failure_message": "Please hold on a second.",
    "params": {
      "model": "gpt-4o-mini"
    },
  }
```

#### Key parameters

-  `api_key`: Create or manage your API key from [OpenAI organization settings](https://platform.openai.com/settings/organization/api-keys).
- `url`: Use the completions endpoint.
- `model`: Refer to [OpenAI models](https://platform.openai.com/docs/models) for available models.

For advanced configuration options, model capabilities, and detailed parameter descriptions, see the [OpenAI API documentation](https://platform.openai.com/docs/api-reference/responses/create).

### Azure OpenAI

Azure OpenAI service provides REST API access to OpenAI's powerful language models through Microsoft Azure's secure infrastructure.

#### Sample configuration

The following example shows a starting `llm` parameter configuration you can use when you [Start a conversational AI agent](../rest-api/join).

```json
"llm": {
    "url": "https://YOUR_RESOURCE_NAME.openai.azure.com/openai/deployments/YOUR_DEPLOYMENT_NAME/chat/completions?api-version=2024-06-01",
    "api_key": "<api_key>",
    "system_messages": [
        {
            "role": "system",
            "content": "You are a helpful chatbot."
        }
    ],
    "max_history": 32,
    "greeting_message": "Hello, how can I assist you today?",
    "failure_message": "Please hold on a second.",
    "params": {
        "model": "ChatGPT-4o-min"
    }
}
```

#### Key parameters

- `api_key`: Find your API key in the Azure portal under your OpenAI resource.
- `url`: Replace `YOUR_RESOURCE_NAME` with your Azure resource name and `YOUR_DEPLOYMENT_NAME` with your model deployment name.
- `model`: Use the deployment name you created in Azure (not the base model name).

For advanced configuration options, deployment setup, and detailed parameter descriptions, see the [Azure OpenAI API documentation](https://learn.microsoft.com/en-us/azure/ai-services/openai/reference).


### Google Gemini

Google Gemini provides advanced multimodal AI capabilities with fast performance and efficient processing for conversational AI applications.

#### Sample configuration

The following example shows a starting `llm` parameter configuration you can use when you [Start a conversational AI agent](../rest-api/join).

```json
"llm": {
   "url": "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:streamGenerateContent?alt=sse&key=<api_key>",
   "system_messages": [
       {
           "parts": [
               {
                   "text": "You are a helpful chatbot"
               }
           ],
           "role": "user"
       }
   ],
   "max_history": 32,
   "greeting_message": "Good to see you!",
   "failure_message": "Hold on a second.",
   "params": {
       "model": "gemini-2.0-flash" 
   },
   "ignore_empty": true,
   "style": "gemini"
}
```

#### Key parameters

- `url`: Note that the API key is passed in the URL query parameter. Get your API key from [Google AI Studio](https://aistudio.google.com/app/apikey).
- `system_messages`: Use `parts` array with `text` objects instead of simple `content` string.
- `model`: Refer to [Gemini models](https://ai.google.dev/gemini-api/docs/models/gemini) for available models.
- `style`: Set to `"gemini"` to use Gemini's message format.
- `ignore_empty`: Set to `true` to handle empty responses appropriately.

For advanced configuration options, model capabilities, and detailed parameter descriptions, see the [Google Gemini API documentation](https://cloud.google.com/docs/authentication/rest).

### Google Vertex AI

Google Vertex AI provides enterprise-grade access to Google's generative AI models with enhanced security, scaling capabilities, and integration with Google Cloud services.

#### Authentication

Using Dynamic API Keys

Google Vertex AI requires dynamic API keys (Access Tokens) to securely access service endpoints. Use a Google Service Account JSON file and the [google.golang.org/api](http://google.golang.org/api) library to generate tokens dynamically.

#### Sample configuration

The following example shows a starting `llm` parameter configuration you can use when you [Start a conversational AI agent](../rest-api/join).

```json
"llm": {
   "url": "https://us-central1-aiplatform.googleapis.com/v1/projects/inbound-coast-460602-g1/locations/us-central1/publishers/google/models/gemini-2.0-flash-001:streamGenerateContent?alt=sse",
   "api_key": "<api_key>",
   "system_messages": [
       {
           "role": "user",
           "parts": [ {"text": "You are a helpful chatbot."} ]
       }
   ],
   "max_history": 32,
   "greeting_message": "Good to see you!",
   "failure_message": "Hold on a second.",
   "params": {
       "model": "gemini-2.0-flash-001"
   },
   "style": "gemini"
}
```

#### Key parameters

- `api_key`: Get your API key from Google Cloud Console with Vertex AI API enabled.
- `url`: Use Vertex AI endpoint and includes your Google Cloud project ID and region in the URL path.
- `model`: Refer to [Vertex AI models](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models) for available models.
- `system_messages`: Use `parts` array with `text` objects instead of simple `content` string.
- `style`: Set to `"gemini"` to use Gemini's message format.

For advanced configuration options, model capabilities, and detailed parameter descriptions, see the [Google Vertex AI API documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/system-instructions).

### Claude

Claude from Anthropic provides helpful, harmless, and honest AI assistance with strong reasoning capabilities and natural conversational abilities.

#### Sample configuration

The following example shows a starting `llm` parameter configuration you can use when you [Start a conversational AI agent](../rest-api/join).

```json
"llm": {
   "url": "https://api.anthropic.com/v1/messages",
   "api_key": "<api_key>",
   "headers": "{\"anthropic-version\":\"2023-06-01\"}",
   "system_messages": [
       {
           "role": "user",
           "content": "You are a Conversational AI Agent, developed by Agora."
       }
   ],
   "max_history": 2,
   "greeting_message": "Good to see you!",
   "failure_message": "Hold on a second.",
   "params": {
       "model": "claude-3-7-sonnet-20250219",
       "max_tokens": 1024
   },
   "style": "anthropic",
   "ignore_empty": true
}
```

#### Key parameters

- `api_key`: Get your API key from [Anthropic Console](https://console.anthropic.com/).
- `url`: Use Anthropic's messages endpoint.
- `headers`: Must include `anthropic-version` header for API compatibility.
- `model`: Refer to [Claude models](https://docs.anthropic.com/en/docs/about-claude/models) for available models.
- `max_tokens`: Must specify maximum tokens in the response.
- `style`: Set to `"anthropic"` to use Claude's message format.
- `ignore_empty`: Set to `true` to handle empty responses appropriately.

For advanced configuration options, model capabilities, and detailed parameter descriptions, see the [Claude API documentation](https://docs.anthropic.com/en/api/messages).

### Dify

Dify is an open-source LLM application development platform that provides a unified API for multiple LLM providers with workflow orchestration and advanced features.

#### Setup and Configuration

To integrate Dify with Agora Conversational AI, install the plugin, configure your Agora project, and create an endpoint:

1. **Install the Dify plugin**
   1. Log in to the [Dify Console](https://console.dify.ai).
   2. Click on **Plugins** in the top navigation bar to open the Marketplace.
   3. Search for the **Agora Conversational AI** plugin.
   4. Click **Install** to install the plugin.
   5. After installation, confirm that the plugin status is **Active** in the Installed Plugins list.

2. **Configure the Agora Console**
   1. Log in to the [Agora Console](https://console.agora.io).
   2. Create a new project or select an existing one.
   3. Enable the **Conversational AI** feature within the selected project.
   4. Record the following information for later use:
      - **App ID** (unique project identifier)
      - **Certificate** (optional, if enabled for enhanced security)
   5. Navigate to the **RESTful API** page and generate an **API Key** and **API Secret** (required for HTTP Basic Authentication).

3. **Create an Endpoint in Dify**
   1. In the Dify console's left-hand navigation panel, click on the **Agora Conversational AI** plugin you just installed.
   2. Click **Create a new API endpoint** to open the configuration page.
   3. Fill in the following details:
      - **App ID** (copied from the Agora Console)
      - **API Key and Secret** (copied from the Agora Console)
      - **TTS** (Text-to-Speech) configuration (e.g., Azure or ElevenLabs)

4. **Save and note the endpoint details**
   1. Save the endpoint configuration. Dify generates an **Endpoint URL**.
   2. Note down this **Endpoint URL** and **API Key** for use in the `llm` configuration.

    ![](/images/conversational-ai/dify-endpoint.png)
    
#### Sample configuration

The following example shows a starting `llm` parameter configuration you can use when you [Start a conversational AI agent](../rest-api/join).

```json
"llm": {
   "url": "https://qv90***.ai-plugin.io/convoai-start",
   "api_key": "<same_api_key_as_in_endpoint_setup>",
   "system_messages": [
       {
           "role": "system",
           "content": "You are a helpful voice assistant."
       }
   ],
   "max_history": 32,
   "greeting_message": "Hello, how can I assist you today?",
   "failure_message": "Hold on a moment.",
   "params": {
       "model": "default"
   },
   "style": "dify"
}
```

#### Key parameters

- `api_key`: Use the same API key configured in your Dify endpoint setup.
- `url`: Use the custom endpoint URL from your Dify marketplace integration.
- `model`: Set to `"default"` to use the model configured in your Dify application.
- `style`: Set to `"dify"` to use Dify's message format and workflow processing.

For advanced configuration options, workflow setup, and detailed parameter descriptions, see the [Agora Conversational AI - Dify Marketplace](https://marketplace.dify.ai/plugins/plutoless/convoai) documentation.

### Custom LLMs

You can integrate any LLM that provides an OpenAI-compatible REST API interface. Your custom service must handle requests and responses in the OpenAI API format. For implementation details and requirements, see the [Custom LLM](../develop/custom-llm) integration guide.

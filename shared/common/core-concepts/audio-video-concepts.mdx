<ProductWrapper notAllowed="voice-calling">

### Basic audio and video interaction workflow

The following figure shows the basic workflow of using the <Vpd k="SDK" /> to implement basic audio and video interaction.
</ProductWrapper>

<ProductWrapper product="voice-calling">
### Basic audio interaction workflow

The following diagram shows the basic workflow of using the <Vpd k="SDK" /> to implement basic audio interaction.
</ProductWrapper>

![orientation_adaptive_locked_landscape](/images/common/basic-audio-and-video.svg)

<Vg k="COMPANY" />  relies on the following fundamental concepts to enable seamless real-time communication:

<PlatformWrapper platform="web">

### Track

A track contains specific audio or video information. It consists of three parts: input source, filter, and output. According to different functions in the RTC process, tracks can be further classified into uplink tracks and downlink tracks.

![Track](/images/common/track.svg)

#### Input source

The input source is the local audio or video data to be published. It can be from a camera, screen capture, or microphone source, or parsed from a media file.

#### Filter 

A filter performs a series of processing operations on audio and video, including pre-processing and post-processing, and transmits the processed audio and video signals to the output. Filters can be connected to multiple input sources or multiple outputs.

    - Pre-processing: Audio/video filters in the sender track, such as virtual background, beautification, echo cancellation, and noise reduction.
    - Post-processing : Audio/video filters in the receiving track, such as super-resolution, and spatial audio effects.

#### Output

Located at the end of the track, such as an encoder, or a renderer.
</PlatformWrapper>

<PlatformWrapper notAllowed="web">
### Audio module

In audio interaction, the main functions of the audio module are as shown in the figure below:

![Audio module functions](/images/common/audio-module.svg)

After you call `registerAudioFrameObserver`, you can obtain the raw audio data at the following observation points in the audio transmission process:

1. Obtain the raw audio data of ear monitoring through the `onEarMonitoringAudioFrame` callback.  
2. Obtain the captured raw audio data through the `onRecordAudioFrame` callback.  
3. Obtain the raw audio playback data of each individual stream through the `onPlaybackAudioFrameBeforeMixing` callback.
4. Obtain the raw audio playback data of all mixed streams through the `onPlaybackAudioFrame` callback.
5. Obtain the raw audio data after mixing the captured and playback audio through the `onMixedAudioFrame` callback.  
    (5) `onMixedAudioFrame` = (2) `onRecordAudioFrame` + (4) `onPlaybackAudioFrame`

#### Audio routing

The audio output device used by the app when playing audio. Common audio routes include wired headphones, earpieces, speakers, Bluetooth headphones, and others.

  The APIs used by the audio module are as follows:

  - Enable local audio collection: `enableLocalAudio`
  - Set local playback device: `setPlaybackDevice`
  - Set up audio routing: `setDefaultAudioRouteToSpeakerphone`

<ProductWrapper notAllowed="voice-calling">

### Video module

The following diagram shows the main functions of the video module in video interaction:

![Video module functions](/images/common/video-module.svg)

The figure shows the following observation points: 

1. `POSITION_POST_CAPTURER_ORIGIN`.
2. `POSITION_POST_CAPTURER`, corresponds to the `onCaptureVideoFrame` callback.
3. `POSITION_PRE_ENCODER`, corresponds to the `onPreEncodeVideoFrame` callback.
4. `POSITION_PRE_RENDERER`, corresponds to the `onRenderVideoFrame` callback.

The APIs used by the video module are as follows:

- Enable local video collection: `enableLocalVideo`
- Local preview: `setupLocalVideo` â†’ `startPreview`
- Video rendering shows: `setupRemoteVideo`

</ProductWrapper>

</PlatformWrapper>
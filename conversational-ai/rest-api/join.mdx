---
title: "Start a conversational AI agent"
sidebar_position: 1
type: docs
platform_selector: false
description: >
  Create and start a Conversational AI agent instance.
---
import RestAPILayout from '@site/src/components/rest-api/RestAPILayout';
import LeftColumn from '@site/src/components/rest-api/LeftColumn';
import RightColumn, { Section } from '@site/src/components/rest-api/RightColumn';
import ParameterList, { Parameter } from '@site/src/components/rest-api/ParameterList';
import PathParameter from '@site/src/components/rest-api/PathParameter';
import { Tabs, TabItem } from '@site/src/components/rest-api/Tabs';

<RestAPILayout>
  <LeftColumn
    title={frontMatter.title}
    method="POST"
    endpoint="https://api.agora.io/api/conversational-ai-agent/v2/projects/{appid}/join"
  >

Use this endpoint to create and start a <Vpd k="AGENT" /> instance.

## Request

### Path parameters

<PathParameter name="appid" type="string" required={true}>
  The App ID of the project.
</PathParameter>

### Request body

<div class="api-mime-type">APPLICATION/JSON</div>

<ParameterList title="BODY" required={true}>
  <Parameter name="name" type="string" required={true}>
    The unique identifier of the agent. The same identifier cannot be used repeatedly.
  </Parameter>

  <Parameter name="properties" type="object" required={true}>
    Configuration details of the agent.

    <Parameter name="channel" type="string" required={true}>
      The name of the channel to join.
    </Parameter>

    <Parameter name="token" type="string" required={true}>
      The authentication token used by the agent to join the channel.
    </Parameter>

    <Parameter name="agent_rtc_uid" type="string" required={true}>
      The user ID of the agent in the channel. A value of `0` means that a random UID is generated and assigned. Set the `token` accordingly.
    </Parameter>

    <Parameter name="remote_rtc_uids" type="array[string]" required={true}>
      A list of user IDs that the agent subscribes to in the channel. Only subscribed users can interact with the agent.  Use `"*"` to subscribe to all users in the channel.
      <Admonition type="info">
      - The `"*"` selector includes all UIDs present in the channel, which may include other AI agents. If you're running multiple agents in the same channel, review the best practices under the `idle_timeout` parameter to avoid unintended behavior and unnecessary usage costs.
      - When using an AI Avatar, subscribing to all users with `"*"` is not supported.
      </Admonition>
    </Parameter>

    <Parameter name="enable_string_uid" type="boolean" defaultValue="false" required={false}>
      Whether to enable String uid:
      <ul>
        <li>`true`: Both agent and subscriber user IDs use strings.</li>
        <li>`false`: Both agent and subscriber user IDs must be integers.</li>
      </ul>
    </Parameter>

    <Parameter name="idle_timeout" type="integer" defaultValue="30" required={false}>
      Sets the timeout after all the users specified in `remote_rtc_uids` are detected to have left the channel. When the timeout value is exceeded, the agent automatically stops and exits the channel. A value of `0` means that the agent does not exit until it is stopped manually.
      <Admonition type="caution" title="Multi-agent use cases">
        If multiple AI agents are active in the same channel and are configured to subscribe to all users using `remote_rtc_uids: ["*"]`, they detect each other's presence.
        As a result, the `idle_timeout` condition, when all other users have left, might never be triggered. This can cause agents to run indefinitely and lead to significant unintended usage.
      </Admonition>
      <Admonition type="tip" title="Agent lifecycle best practice">
        For precise and reliable control over the agent's lifecycle, use the [`leave`](../rest-api/leave) API to terminate the agent as soon as its task is complete.
      </Admonition>
    </Parameter>

    <Parameter name="advanced_features" type="object" required={false}>
      Advanced features configuration.

      <Parameter name="enable_aivad" type="boolean" defaultValue="false" required={false}>
        Whether to enable the intelligent interruption handling function (AIVAD). This feature is currently available only for English.
      </Parameter>

      <Parameter name="enable_mllm" type="boolean" defaultValue="false" required={false}>
        Enable Multimodal Large Language Model. Enabling MLLM automatically disables ASR, LLM, and TTS. When you set this parameter to true, `enable_aivad` is also disabled.
      </Parameter>

      <Parameter name="enable_rtm" type="boolean" defaultValue="false" required={false}>
        Whether to enable the Signaling (RTM) service. When enabled, the agent can combine the capabilities provided by Signaling to implement advanced functions, such as delivering [custom information](../develop/custom-information).
        <Admonition type="info">
        Before enabling the Signaling service, make sure the token includes both RTC and RTM privileges. When an agent joins an RTM channel, it reuses the token specified in the `token` field. For more information, see ["How can I generate a token with both RTC and Signaling privileges?"](/help/integration-issues/rtc_rtm_token).
        </Admonition>
      </Parameter>
    </Parameter>

    <Parameter name="asr" type="object" required={false}>
      Automatic Speech Recognition (ASR) configuration.

      <Parameter name="language" type="string" defaultValue="en-US" required={false}>
        The BCP-47 language tag identifying the primary language used for agent interaction. If `params` contains a vendor-specific language code, it takes precedence over this setting.
      </Parameter>

      <Parameter name="vendor" type="string" defaultValue="ares" possibleValues="ares,microsoft,deepgram,openai" required={false}>
      ASR provider:
      - `ares`: [Adaptive Recognition Engine for Speech](/conversational-ai/models/asr/ares)
      - `microsoft`: [Microsoft Azure](/conversational-ai/models/asr/microsoft)
      - `deepgram`: [Deepgram](/conversational-ai/models/asr/deepgram)
      - `openai`: [OpenAI (Beta)](/conversational-ai/models/asr/openai)
      - `speechmatics`: [Speechmatics](/conversational-ai/models/asr/speechmatics)

      </Parameter>

      <Parameter name="params" type="object" required={true}>
        The configuration parameters for the ASR vendor. See [ASR Overview](/conversational-ai/models/asr/overview) for details.
      </Parameter>
    </Parameter>
  
    <Parameter name="tts" type="object" required={true}>
      Text-to-speech (TTS) module configuration.

      <Parameter name="vendor" type="string" required={true} possibleValues="microsoft,elevenlabs,cartesia,openai,humeai">
        TTS provider. 
        - `microsoft`: [Microsoft Azure](/conversational-ai/models/tts/microsoft)
        - `elevenlabs`: [ElevenLabs](/conversational-ai/models/tts/elevenlabs)
        - `cartesia` : [Cartesia](/conversational-ai/models/tts/cartesia)
        - `openai`: [OpenAI](/conversational-ai/models/tts/openai)
        - `humeai`: [Hume AI](/conversational-ai/models/tts/humeai)
        - `rime`: [Rime (Beta)](/conversational-ai/models/tts/rime)
        - `minimax`: [Minimax](/conversational-ai/models/tts/minimax)
        - `fishaudio`: [Fish AUdio (Beta)](/conversational-ai/models/tts/fish-audio)
        - `groq`: [Groq (Beta)](/conversational-ai/models/tts/groq)
      </Parameter>

      <Parameter name="params" type="object" required={true}>
        The configuration parameters for the TTS vendor. See [TTS Overview](/conversational-ai/models/tts/overview) for details.
      </Parameter>

      <Parameter name="skip_patterns" type="array[integer]" required={false}>
        Controls whether the TTS module skips bracketed content when reading LLM response text. This prevents the agent from vocalizing structural prompt information like tone indicators, action descriptions, and system prompts, creating a more natural and immersive listening experience. Enable this feature by specifying one or more values:
        
        - `1`: Skip content in Chinese parentheses `（）`
        - `2`: Skip content in Chinese square brackets `【】`
        - `3`: Skip content in parentheses `( )`
        - `4`: Skip content in square brackets `[ ]`
        - `5`: Skip content in curly braces `{ }`

        <Admonition type="info">
        - **Nested brackets**: When input text contains nested brackets and multiple bracket types are configured to be skipped, the system processes only the outermost brackets. The system matches from the beginning of the text and skips the first outermost bracket pair that meets the skip rule, including all nested content.
        - **Agent memory**: The agent's short-term memory always contains the complete, unfiltered LLM text, regardless of live captioning settings.
        - **Real-time subtitles**: When enabled, subtitles exclude filtered content during TTS playback but restore the complete text after each sentence finishes.
        </Admonition>
      </Parameter>
    </Parameter>

    <Parameter name="llm" type="object" required={true}>
      Large language model (LLM) configuration.

      <Parameter name="url" type="string" required={true}>
        The LLM callback address.
      </Parameter>

      <Parameter name="api_key" type="string" defaultValue="" required={false}>
        The LLM verification API key. The default value is an empty string. Ensure that you enable the API key in a production environment.
      </Parameter>

      <Parameter name="system_messages" type="array[object]" required={false}>
        A set of predefined information used as input to the LLM, including prompt words and examples.
      </Parameter>

      <Parameter name="params" type="object" required={false}>
        Additional LLM configuration parameters, such as the `model` used, and the maximum token limit. For details about each supported LLM, refer to [Supported LLMs](/conversational-ai/models/llm/overview#supported-llms).
      </Parameter>

      <Parameter name="max_history" type="integer" defaultValue="32" required={false}>
        The number of conversation history messages cached in the custom LLM. The minimum value is `1`. History includes user and agent dialog messages, tool call information, and timestamps. Agent and user messages are recorded separately.
      </Parameter>

      <Parameter name="input_modalities" type="array[string]" defaultValue='["text"]' required={false}>
        LLM input modalities: 
        - `["text"]`: Text only
        - `["text", "image"]`: Text plus image; requires the selected LLM to support visual input
      </Parameter>

      <Parameter name="output_modalities" type="array[string]" defaultValue='["text"]' required={false}>
        LLM output modalities: 
        - `["text"]`: The output text is converted to speech by the TTS module and then published to the RTC channel.
        - `["audio"]`: Voice only. Voice is published directly to the RTC channel.
        - `["text", "audio"]`: Text plus voice. Write your own logic to process the output of LLM as needed.
      </Parameter>

      <Parameter name="greeting_message" type="string" required={false}>
        Agent greeting. If provided, the first user in the channel is automatically greeted with the message upon joining.
      </Parameter>

      <Parameter name="failure_message" type="string" required={false}>
        Prompt for agent activation failure. If provided, it is returned through TTS when the custom LLM call fails.
      </Parameter>

      <Parameter name="vendor" type="string" required={false}>
        LLM provider, supports the following settings:
        - `custom`: Custom LLM. When you set this option, the agent includes the following fields, in addition to `role` and `content` when making requests to the custom LLM:
          - `turn_id`: A unique identifier for each conversation turn. It starts from `0` and increments with each turn. One user-agent interaction corresponds to one `turn_id`.
          - `timestamp`: The request timestamp, in milliseconds.
      </Parameter>

      <Parameter name="style" type="string" defaultValue="openai" possibleValues="openai,gemini,anthropic,dify" required={false}>
        The request style for chat completion:
        - `openai`: For OpenAI and OpenAI-compatible APIs
        - `gemini`: For Google Gemini and Google Vertex API format
        - `anthropic`: For Anthropic Claude API format
        - `dify`: For Dify API format

        For details, refer to [Supported LLMs](/conversational-ai/models/llm/overview#supported-llms).
      </Parameter>
    </Parameter>

    <Parameter name="mllm" type="object" required={false}>
      Multimodal Large Language Model (MLLM) configuration for real-time audio and text processing.

      <Parameter name="url" type="string" required={true}>
        The MLLM WebSocket URL for real-time communication.
      </Parameter>

      <Parameter name="api_key" type="string" required={true}>
        The API key used for MLLM authentication.
      </Parameter>

      <Parameter name="messages" type="array[object]" required={false}>
        Array of conversation items used for short-term memory management. Uses the same structure as `item.content` from the <a href="https://platform.openai.com/docs/api-reference/realtime-client-events/conversation/item/create">OpenAI Realtime API</a>.
      </Parameter>

      <Parameter name="params" type="object" required={false}>
        Additional MLLM configuration parameters.
        - **Modalities override**: The `modalities` setting in params is overridden by `input_modalities` and `output_modalities`.
        - **Turn detection override**: The `turn_detection` setting in params is overridden by the `turn_detection` section outside of `mllm`.

        See [MLLM Overview](../models/mllm/overview) for details.
      </Parameter>

      <Parameter name="max_history" type="integer" defaultValue="32" required={false}>
        The number of conversation history messages cached in the MLLM. The minimum value is `1`. Cannot exceed the model's context window.
      </Parameter>

      <Parameter name="input_modalities" type="array[string]" defaultValue='["audio"]' required={false}>
        MLLM input modalities:
        - `["audio"]`: Audio only
        - `["audio", "text"]`: Audio plus text
      </Parameter>

      <Parameter name="output_modalities" type="array[string]" defaultValue='["text", "audio"]' required={false}>
        MLLM output modalities:
        - `["text", "audio"]`: Text plus audio
      </Parameter>

      <Parameter name="greeting_message" type="string" required={false}>
        Agent greeting message. If provided, the first user in the channel is automatically greeted with this message upon joining.
      </Parameter>

      <Parameter name="vendor" type="string" required={false} possibleValues="openai">
        MLLM provider. Currently supports:
        - `openai`: OpenAI Realtime API
      </Parameter>

      <Parameter name="style" type="string" defaultValue="openai" possibleValues="openai" required={false}>
        The request style for MLLM completion:
        - `openai`: For OpenAI Realtime API format 
      </Parameter>
    </Parameter>  

    <Parameter name="avatar" type="object" required={false}>  
      Avatar configuration.

      <Parameter name="enable" type="boolean" defaultValue="false" required={false}>  
        Whether to enable the avatar function for the agent. To enable, set to `true` and configure the `vendor` and `params` fields.
      </Parameter> 

      <Parameter name="vendor" type="string" possibleValues="akool,heygen" required={false}>  
        Avatar vendor. Supports the following values:  
        - `akool`: [Akool](/conversational-ai/models/avatar/akool)
        - `heygen`: [HeyGen](/conversational-ai/models/avatar/heygen)
      </Parameter> 

      <Parameter name="params" type="object" required={false}>  
        The configuration parameters for the avatar vendor. See [AI Avatar Overview](/conversational-ai/models/avatar/overview) for details.
      </Parameter> 

    </Parameter>  

    <Parameter name="vad" type="object" required={false} deprecated>
      Voice Activity Detection (VAD) configuration.

      <Admonition type="caution" title="Deprecation Notice">
      The `vad` configuration section is deprecated and will not be supported in future versions. When the same parameter exists in both `vad` and `turn_detection` sections, `turn_detection` parameters have higher priority. <Vg k="COMPANY" /> recommends migrating to `turn_detection` configuration for voice activity detection settings.
      </Admonition>

      <Parameter name="interrupt_duration_ms" type="number" defaultValue="160" required={false}>
        The amount of time in milliseconds that the user's voice must exceed the VAD threshold before an interruption is triggered.
      </Parameter>

      <Parameter name="prefix_padding_ms" type="integer" defaultValue="800" required={false}>
        The extra forward padding time in milliseconds before the processing system starts to process the speech input. This padding helps capture the beginning of the speech.
      </Parameter>

      <Parameter name="silence_duration_ms" type="integer" defaultValue="640" required={false}>
        The duration of audio silence in milliseconds. If no voice activity is detected during this period, the agent assumes that the user has stopped speaking.
      </Parameter>

      <Parameter name="threshold" type="number" defaultValue="0.5" required={false}>
        Identification sensitivity determines the level of sound in the audio signal that is considered voice activity. The value range is `(0.0, 1.0)`. Lower values make it easier for the agent to detect speech, and higher values ignore weak sounds.
      </Parameter>
    </Parameter>

    <Parameter name="turn_detection" type="object" required={false}>
      Conversation turn detection settings.

      <Parameter name="type" type="string" defaultValue="agora_vad" possibleValues="agora_vad, server_vad,semantic_vad" required={false}>
        Turn detection mechanism.
        - `agora_vad`:  Agora VAD.
        - `server_vad`: The model detects the start and end of speech based on audio volume and responds at the end of user speech. Only available when `mllm` is enabled and OpenAI is selected.
        - `semantic_vad`: Uses a turn detection model in conjunction with VAD to semantically estimate whether the user has finished speaking, then dynamically sets a timeout based on this probability for more natural conversations. Only available when `mllm` is enabled and OpenAI is selected.
      </Parameter>

      <Parameter name="interrupt_mode" type="string" defaultValue="interrupt" required={false}>
        Sets the agent's behavior when human voice interrupts the agent while it is interacting (speaking or thinking). Choose from the following values:

          - `interrupt`: The agent immediately stops the current interaction and processes the human voice input.
          - `append`: The agent completes the current interaction, then processes the human voice input.
          - `ignore`: The agent discards the human voice input without processing or storing it in the context.
          - `keyword`: The agent stops its current interaction after detecting any of the keywords specified in `turn_detection.interrupt_keywords`.
          - `adaptive`: The agent dynamically increases the voice continuity threshold while speaking to reduce accidental interruptions.

          <Admonition type="info">
          Only the `interrupt` mode is supported when you integrate a `mllm`.
          </Admonition>
      </Parameter>  

      <Parameter name="interrupt_duration_ms" type="number" defaultValue="160" required={false}>
        The amount of time in milliseconds that the user's voice must exceed the VAD threshold before an interruption is triggered.
      </Parameter>

      <Parameter name="interrupt_keywords" type="array[string]" required={false}>
          Specifies the list of keywords that trigger an interruption when the `turn_detection.interrupt_mode` is set to `"keyword"`.

          When the agent detects any of these keywords in the user's speech, it immediately stops its current interaction and processes the new input.
      </Parameter>   

      <Parameter name="prefix_padding_ms" type="integer" defaultValue="800" required={false}>
        The extra forward padding time in milliseconds before the processing system starts to process the speech input. This padding helps capture the beginning of the speech.
      </Parameter>

      <Parameter name="silence_duration_ms" type="integer" defaultValue="640" required={false}>
        The duration of audio silence in milliseconds. If no voice activity is detected during this period, the agent assumes that the user has stopped speaking.
      </Parameter>

      <Parameter name="threshold" type="number" defaultValue="0.5" required={false}>
        Identification sensitivity determines the level of sound in the audio signal that is considered voice activity. The value range is `(0.0, 1.0)`. Lower values make it easier for the agent to detect speech, and higher values ignore weak sounds.
      </Parameter>

      <Parameter name="create_response" type="boolean" defaultValue="true" required={false}>
        Whether to automatically generate a response when a VAD stop event occurs. Only available in `server_vad` and `semantic_vad` modes when using OpenAI Realtime API.
      </Parameter>

      <Parameter name="interrupt_response" type="boolean" defaultValue="true" required={false}>
        Whether to automatically interrupt any ongoing response when a VAD start event occurs. Only available in `server_vad` and `semantic_vad` modes when using OpenAI Realtime API.
      </Parameter>

      <Parameter name="eagerness" type="string" defaultValue="auto" possibleValues="auto,low,high" required={false}>
        The eagerness of the model to respond:
        - `auto`: Equivalent to medium
        - `low`: Wait longer for the user to continue speaking
        - `high`: Respond more quickly
        
        Only available in `semantic_vad` mode when using OpenAI Realtime API.
      </Parameter>

    </Parameter>

    <Parameter name="parameters" type="object" required={false}>
      Agent configuration parameters.
      <Parameter name="silence_config" type="object" required={false}>
        Settings related to agent silence behavior.
        <Admonition type="info">
        `silence_config` does not apply when you integrate a `mllm`.
        </Admonition>
        <Parameter name="timeout_ms" type="integer" possibleValues="0 to 60000" defaultValue="0" required={false}>
          Specifies the maximum duration (in milliseconds) that the agent can remain silent. 
          After the agent is successfully created and the user joins the channel, any time during which the agent is not listening, thinking, or speaking is considered silent time. When the silent time reaches the specified value, the agent broadcasts a silent reminder message. This feature is useful for prompting users when they become inactive.
          - `0`: Disables the silent reminder feature.
          - `(0, 60000]`: Enables the silent reminder. You must also set `content`; otherwise, the configuration is invalid.
        </Parameter>
        <Parameter name="action" type="string" defaultValue="speak" required={false}>
          Specifies how the agent behaves when the silent timeout is reached. Valid values:
          - `speak`: Uses the TTS module to announce the silent prompt (`content`).
          - `think`: Appends the silent prompt (`content`) to the context and passes it to the LLM.
        </Parameter>
        <Parameter name="content" type="string" required={false}>
          Specifies the silent prompt message. The message use depends on the value of `action` parameter.
        </Parameter>
      </Parameter>

      <Parameter name="data_channel" type="string" defaultValue="datastream" required={false}>
      Agent data transmission channel:
      - `rtm`: Use RTM transmission. This configuration takes effect only when `advanced_features.enable_rtm` is `true`.
      - `datastream`: Use RTC data stream transport.
      </Parameter>

      <Parameter name="enable_metrics" type="boolean" defaultValue="false" required={false}>
      Whether to receive agent performance data:
      - `true`: Receive agent performance data.
      - `false`: Do not receive agent performance data.
      
      This setting only takes effect when `advanced_features.enable_rtm` is `true`. See [Listen to agent events](../develop/agent-events) to learn how to use client components to receive agent performance data.
      </Parameter>

      <Parameter name="enable_error_message" type="boolean" defaultValue="false" required={false}>
      Whether to receive agent error events:
      - `true`: Receive agent error events.
      - `false`: (Default) Do not receive agent error events.
      
      This setting only takes effect when `advanced_features.enable_rtm` is `true`. See [Listen to agent events](../develop/agent-events) to learn how to use client components to receive agent error events.
      </Parameter>      
    </Parameter>    
  </Parameter>
</ParameterList>

## Response

- If the returned status code is `200`, the request was successful. The response body contains the result of the request.

  <ParameterList title="OK">
    <Parameter name="agent_id" type="string">
      Unique id of the agent instance
    </Parameter>
    
    <Parameter name="create_ts" type="integer">
      Timestamp of when the agent was created
    </Parameter>
    
    <Parameter 
      name="status" 
      type="string" 
      possibleValues="IDLE, STARTING, RUNNING, STOPPING, STOPPED, RECOVERING, FAILED"
    >
      Current status.
      <ul>
        <li>`IDLE` (0): Agent is idle.</li>
        <li>`STARTING` (1): The agent is being started.</li>
        <li>`RUNNING` (2): The agent is running.</li>
        <li>`STOPPING` (3): The agent is stopping.</li>
        <li>`STOPPED` (4): The agent has exited.</li>
        <li>`RECOVERING` (5): The agent is recovering.</li>
        <li>`FAILED` (6): The agent failed to execute.</li>
      </ul>
    </Parameter>
  </ParameterList>

- If the returned status code is not `200`, the request failed. The response body includes the `detail` and `reason` for failure. Refer to [status codes](../rest-api/reference#response-status-codes) to understand the possible reasons for failure.

</LeftColumn>

<RightColumn>

<Section title="Authorization">
This endpoint requires [Basic Auth](../rest-api/restful-authentication).
</Section>

<Section title="Request example">

  <Tabs groupId="code-examples">
    <TabItem value="curl" label="curl" default>
      ```bash
      curl --request post \
      --url https://api.agora.io/api/conversational-ai-agent/v2/projects/:appid/join \
      --header 'Authorization: Basic <your_base64_encoded_credentials>' \
      --data '
      {
          "name": "unique_name",
          "properties": {
              "channel": "channel_name",
              "token": "token",
              "agent_rtc_uid": "1001",
              "remote_rtc_uids": [
                  "1002"
              ],
              "idle_timeout": 120,
              "advanced_features": {
                  "enable_aivad": true
              },
              "llm": {
                  "url": "https://api.openai.com/v1/chat/completions",
                  "api_key": "<your_llm_key>",
                  "system_messages": [
                      {
                          "role": "system",
                          "content": "You are a helpful chatbot."
                      }
                  ],
                  "max_history": 32,
                  "greeting_message": "Hello, how can I assist you today?",
                  "failure_message": "Please hold on a second.",
                  "params": {
                      "model": "gpt-4o-mini"
                  }
              },
              "tts": {
                  "vendor": "microsoft",
                  "params": {
                      "key": "<your_tts_api_key>",
                      "region": "eastus",
                      "voice_name": "en-US-AndrewMultilingualNeural"
                  }
              },
              "asr": {
                  "language": "en-US"
              }
          }
      }'
      ```
    </TabItem>
    <TabItem value="python" label="Python">

    ```python
    import requests
    import json

    url = "https://api.agora.io/api/conversational-ai-agent/v2/projects/:appid/join"

    headers = {"Authorization": "Basic <your_base64_encoded_credentials>"}

    data = {
        "name": "unique_name",
        "properties": {
            "channel": "channel_name",
            "token": "token",
            "agent_rtc_uid": "1001",
            "remote_rtc_uids": ["1002"],
            "idle_timeout": 120,
            "advanced_features": {
                "enable_aivad": True
            },
            "llm": {
                "url": "https://api.openai.com/v1/chat/completions",
                "api_key": "<your_llm_key>",
                "system_messages": [
                    {
                        "role": "system",
                        "content": "You are a helpful chatbot."
                    }
                ],
                "max_history": 32,
                "greeting_message": "Hello, how can I assist you today?",
                "failure_message": "Please hold on a second.",
                "params": {
                    "model": "gpt-4o-mini"
                }
            },
            "tts": {
                "vendor": "microsoft",
                "params": {
                    "key": "<your_tts_api_key>",
                    "region": "eastus",
                    "voice_name": "en-US-AndrewMultilingualNeural"
                }
            },
            "asr": {
                "language": "en-US"
            }
        }
    }

    response = requests.post(url, headers=headers, data=json.dumps(data))

    print(response.status_code)
    print(response.json())
    ```
    </TabItem>

    <TabItem value="node" label="Node.js">

    ```js
    const axios = require("axios");

    const url = "https://api.agora.io/api/conversational-ai-agent/v2/projects/:appid/join";

    const headers = {
      "Authorization": "Basic <your_base64_encoded_credentials>"
    };

    const data = {
      name: "unique_name",
      properties: {
        channel: "channel_name",
        token: "token",
        agent_rtc_uid: "1001",
        remote_rtc_uids: ["1002"],
        idle_timeout: 120,
        advanced_features: {
          enable_aivad: true
        },
        llm: {
          url: "https://api.openai.com/v1/chat/completions",
          api_key: "<your_llm_key>",
          system_messages: [
            {
              role: "system",
              content: "You are a helpful chatbot."
            }
          ],
          max_history: 32,
          greeting_message: "Hello, how can I assist you today?",
          failure_message: "Please hold on a second.",
          params: {
            model: "gpt-4o-mini"
          }
        },
        tts: {
          vendor: "microsoft",
          params: {
            key: "<your_tts_api_key>",
            region: "eastus",
            voice_name: "en-US-AndrewMultilingualNeural"
          }
        },
        asr: {
          language: "en-US"
        }
      }
    };

    axios
      .post(url, data, { headers })
      .then(response => {
        console.log("Status:", response.status);
        console.log("Response:", response.data);
      })
      .catch(error => {
        console.error("Error:", error.response ? error.response.data : error.message);
      });
    ```
    </TabItem>
  </Tabs>

</Section>
<Section title="Response example">

  ```json
  {
    "agent_id": "1NT29X10YHxxxxxWJOXLYHNYB",
    "create_ts": 1737111452,
    "status": "RUNNING"
  }
  ```

</Section>
  </RightColumn>
</RestAPILayout>
---
title: 'Product overview'
sidebar_position: 1
platform_selector: false
description: >
  Add natural and responsive voice AI experiences to your app with the LLM of your choice.
---

<ProductOverview
    title="Conversational AI Engine"
    img="/images/conversational-ai/conversational-ai.png"
    beta={true}
    restQuickStartLink="/conversational-ai/get-started/quickstart"
    apiReferenceLink="/conversational-ai/rest-api/agent/join"
    linkButtons={[
        {
            label: "Agent quickstart",
            link: "/conversational-ai/get-started/quickstart",
            description: "Build a backend server to manage conversational AI agents",
            variant: "default",
            group: "primary"
        },
        {
            label: "API reference",
            link: "/conversational-ai/get-started/quickstart-rest",
            description: "Learn how to call the Conversational AI Engine RESTful APIs to start and stop an AI agent",
            variant: "default",
            group: "primary"
        }
    ]}
    productFeatures={[
        {
            title: "Ultra-low latency",
            content: "Deeply optimized pipeline reduces AI response latency to as low as 650 ms end-to-end. The engine handles ASR, LLM, and TTS processing with minimal overhead, enabling natural conversation flow.",
            link: ""
        },
        {
            title: "Cascading or multimodal workflows",
            content: "Choose the STT-LLM-TTS pipeline for full control over each component, or use multimodal voice-to-voice models for faster response times and natural tone understanding. Switch between approaches as your needs evolve.",
            link: ""
        },
        {
            title: "Bring your own AI models",
            content: "Mix and match from 15+ ASR, LLM, TTS, and MLLM providers. Use Agora's built-in ARES ASR, OpenAI, Anthropic, Google Gemini, Azure, Deepgram, ElevenLabs, and more—or connect your own.",
            link: ""
        },
        {
            title: "Custom LLM integration",
            content: "Connect custom LLM endpoints supporting OpenAI, Anthropic, or Gemini formats. Implement RAG for knowledge retrieval, function calling, tool invocation, and multi-agent workflows with full control over prompts and context.",
            link: "/conversational-ai/develop/custom-llm"
        },
        {
            title: "Intelligent voice interruption",
            content: "Adaptive turn detection with multiple interrupt modes (interrupt, append, ignore, keyword-based) lets users naturally interrupt the agent. Configure thresholds and behavior to match your use case.",
            link: "/conversational-ai/develop/interrupt-agent"
        },
        {
            title: "Flexible client connectivity",
            content: "Connect users through native apps (iOS, Android, Web) or standard telephony (PSTN/SIP). Add visual avatars from third-party providers for enhanced engagement, or go voice-only for phone-based interactions.",
            link: ""
        },
        {
            title: "Network resilience at scale",
            content: "AI-optimized transmission maintains stable conversations with up to 80% packet loss. Handles brief network disconnects (3-5s) without dropping context or interrupting the conversation.",
            link: ""
        },
        {
            title: "Multimodal input support",
            content: "Process voice, video, text, and image inputs seamlessly. Deploy on iOS, Android, Web, and embedded hardware platforms with consistent cross-platform behavior.",
            link: ""
        },

    ]}

>

<Vg k="COMPANY" />'s <Vpd k="NAME" /> is a cloud-based orchestration that enables real-time voice-ai, video, and avatar interactions between users and AI-driven agents within an <Vg k="COMPANY" /> channel. Building a conversational AI application requires two core components: a **Client App** (the user-facing application where end-users interact with AI agents through voice, video, text, or avatars) and an **Agent Manager** (your backend server that handles invoking <Vg k="COMPANY" />'s orchestration to deploy agents into channels).

The engine supports two distinct workflow approaches to power your agents. The **cascading workflow (STT-LLM-TTS)** chains three specialized AI services—Speech-to-Text, Large Language Model, and Text-to-Speech—giving you full control over each component and the flexibility to mix providers based on cost, speed, or quality needs. The **multimodal-LLM workflow (voice-to-voice)** uses a single AI model that processes voice input directly and generates voice output, enabling faster response times and more natural conversations that understand tone and emotion.

Within the orchestration, you can define custom LLM endpoints to connect existing models, add RAG for knowledge retrieval, integrate custom tools, or build multi-agent flows. This architecture lets you create AI-driven smart assistants, intelligent customer service agents, smart hardware devices, interactive dialogue systems, and collaborative voice-driven tools across customer support, education, entertainment, healthcare, and beyond.

</ProductOverview>
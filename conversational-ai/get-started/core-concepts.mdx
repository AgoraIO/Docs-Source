---
title: 'Core concepts'
sidebar_position: 2
type: docs
platform_selector: false
description: >
  Understand the foundational concepts behind Conversational AI Engine.
---

An overview of key concepts you need to build conversational AI applications with <Vg k="COMPANY" />.

## What is Conversational AI Engine?

<Vpd k="NAME" /> is a cloud-based orchestration that enables real-time voice, video, and avatar interactions between users and AI agents within <Vg k="COMPANY" /> channels. It provides the infrastructure you need to build agents that can hear, think, and respond naturally—handling the complex coordination between speech recognition, language models, and speech synthesis so you can focus on creating intelligent conversational experiences.

The engine supports two distinct AI workflows:
- **Cascading Pipeline (STT-LLM-TTS)**: Chains specialized AI services for full control over each component
- **Multimodal LLM (Voice-to-Voice)**: Uses a single AI model for faster, more natural conversations

You bring the AI models and conversational logic. The engine handles real-time audio transport, session management, and orchestration at scale.

## Understanding Real-Time Communication (RTC)

Real-Time Communication (RTC) enables near-instant audio and video exchange between participants. Unlike HTTP (designed for request-response web browsing), RTC protocols are optimized for continuous media streams with minimal latency—critical for natural conversations where delays break the flow.

WebRTC is the browser-native technology that powers real-time media on the web. All major browsers support it, making it the standard for voice and video applications. But WebRTC is complex to operate at scale—handling NAT traversal, codec negotiation, network adaptation, and quality management requires significant infrastructure.

<Vg k="COMPANY" />'s SD-RTN™ (Software Defined Real-Time Network) handles this operational complexity. It routes audio and video with ultra-low latency, automatically adapts to changing network conditions, and maintains stable connections even with packet loss. When you use <Vpd k="NAME" />, your AI agents join channels through this same battle-tested infrastructure that powers millions of real-time sessions daily.

## Why use Conversational AI Engine?

<Vpd k="NAME" /> differentiates itself through several key advantages:

**Ultra-low latency**: End-to-end response times as low as 650ms make conversations feel natural. The engine's optimized pipeline minimizes overhead between speech recognition, LLM processing, and speech synthesis.

**Bring your own models**: Mix and match from 15+ providers for ASR, LLM, TTS, and MLLM. Use <Vg k="COMPANY" />'s built-in ARES ASR, OpenAI, Anthropic, Google Gemini, Azure, Deepgram, ElevenLabs, and more—or connect your own custom endpoints.

**Choose your workflow**: Start with the cascading pipeline for full control and easy debugging, then switch to multimodal voice-to-voice when you need the fastest response times. Both workflows use the same API.

**Network resilience at scale**: AI-optimized transmission maintains stable conversations with up to 80% packet loss. Handles brief network disconnects (3-5s) without dropping context or interrupting the conversation.

**Focus on logic, not infrastructure**: You define what your agent should say and how it should behave. The engine handles audio transport, session management, token generation, and scaling—so you can spend your time on the conversational experience, not WebRTC debugging.

## Core concepts

Understanding these fundamental concepts will help you build conversational AI applications effectively.

### Agora channel

A channel is a virtual room identified by a name (e.g., `"support-room-123"` or `"user-456-session"`). Both end-users and AI agents join channels to exchange real-time audio, video, and data. Think of it like a video conference room—everyone who joins the same channel can communicate with each other.

Channels are created automatically when the first participant joins and exist as long as participants remain connected. You control who can join by generating RTC tokens scoped to specific channel names.

In a typical conversational AI scenario:
1. Your client app joins a channel using the Agora SDK
2. Your backend deploys an AI agent into the same channel
3. The agent and user exchange audio in real-time through the channel
4. When the conversation ends, both leave the channel

### RTC token

An RTC token is a temporary authentication credential that grants permission to join a specific channel. Tokens prevent unauthorized access to your channels and control what actions participants can take (publish audio/video, subscribe to streams).

**Key characteristics:**
- Generated server-side using your App ID and App Certificate
- Scoped to a specific channel name and user UID
- Expire after a set time (typically 1-24 hours)
- Include a role (publisher or subscriber)

Your backend generates RTC tokens for both human users and AI agents. Never expose your App Certificate to client applications—tokens should always be generated server-side and delivered to clients through your authenticated API.

**Security model:**
```
Client requests to join channel "room-123"
         ↓
Your backend verifies user is authorized
         ↓
Backend generates RTC token for channel "room-123" and user's UID
         ↓
Client uses token to join channel through Agora SDK
```

### User ID (UID)

A unique identifier for each participant in a channel. UIDs can be integers (`0` to `2^32-1`) or strings (up to 255 characters). Users have UIDs, and your agent has a UID too.

**Why UIDs matter:**
- Clients use UIDs to identify who's speaking
- You specify which UIDs the agent should listen to
- UIDs must be unique within a channel

**Example in a customer support scenario:**
- Customer UID: `1002`
- Support agent (AI) UID: `1001`
- Supervisor (human, optional) UID: `1003`

When configuring your agent, you tell it to listen to specific UIDs via the `remote_rtc_uids` parameter. The agent only processes audio from those UIDs—giving you control over who can interact with the agent.

### Agent lifecycle

Agents go through a lifecycle from creation to cleanup:

1. **Start**: Your backend calls <Vg k="COMPANY" />'s REST API (`/join`) with agent configuration. The API returns immediately with an `agent_id`.

2. **Joining**: <Vg k="COMPANY" /> deploys an agent instance in the cloud. The agent authenticates with its RTC token and joins the specified channel as a participant.

3. **Running**: Agent is now active in the channel. It listens to audio from specified user UIDs, processes audio through the AI pipeline (STT → LLM → TTS or MLLM), and speaks responses back into the channel.

4. **Idle**: If no audio is detected for the `idle_timeout` period (configurable), the agent remains in the channel but may eventually time out and leave.

5. **Stop**: Your backend calls the REST API (`/leave`) to stop the agent, or the agent times out. The agent leaves the channel and resources are cleaned up.

**Timeline:** The entire start-to-running process takes 1-3 seconds. Users typically hear the agent's greeting message within 2-3 seconds of the agent starting.

**Important**: Store the `agent_id` returned from the start API—you'll need it to stop, update, or query the agent's status.

### Remote RTC UIDs

The `remote_rtc_uids` parameter is a list of user UIDs your agent should listen to. The agent **only** processes audio from users in this list. If a user's UID isn't included, the agent won't hear them.

**Why this matters:**
- **Privacy**: Prevents the agent from listening to users it shouldn't hear
- **Cost control**: Agent only processes audio from intended participants
- **Multi-party scenarios**: You can have multiple users in a channel but only let the agent interact with specific ones

**Example configurations:**

Single user:
```json
{
  "remote_rtc_uids": ["1002"]
}
```

Multiple users (group conversation):
```json
{
  "remote_rtc_uids": ["1002", "1003", "1004"]
}
```

**Common mistake**: Forgetting to add a user's UID to this list, then wondering why the agent doesn't respond to them.

## How does Conversational AI Engine work?

Understanding the architecture helps you design better applications and debug issues effectively.

### Two-component architecture

Building a conversational AI application requires two core components:

**1. Client App (Frontend)**

The user-facing application where end-users interact with AI agents through voice, video, or text. This can be:
- Web application (React, Vue, vanilla JavaScript)
- Native mobile app (iOS, Android)
- PSTN/SIP telephony connection
- IoT or embedded device

Users join channels using the <Vg k="COMPANY" /> SDK. The client handles audio capture from the user's microphone, sends it to the channel, receives audio from the agent, and plays it through speakers.

**2. Agent Manager (Backend Server)**

Your backend server that manages the agent lifecycle. It:
- Authenticates with <Vg k="COMPANY" />'s RESTful API using Customer ID and Customer Secret
- Generates RTC tokens for agents to join channels
- Deploys agents into channels by calling the `/join` API
- Defines the AI workflow (which models to use, system prompts, voices)
- Handles agent configuration (interruption behavior, idle timeouts)
- Stops agents when conversations end

**Why separate?**

Security. Your backend holds sensitive credentials that must never be exposed to client applications:
- **Customer ID and Customer Secret**: Full API access to your <Vg k="COMPANY" /> project
- **App Certificate**: Used to generate RTC tokens

The backend acts as a trusted intermediary that controls who can create agents and when. Clients never have direct access to agent management APIs—they request agents through your authenticated backend, which enforces your business logic and access control.

[IMAGE: Architecture diagram
- Left box: Client Apps (mobile, web, phone icons) labeled "End Users"
- Center: Agora Cloud (SD-RTN network) with "Channels" 
- Right box: Your Backend Server labeled "Agent Manager"
- Arrows: Users → Join Channel, Backend → Deploy Agent via API, Agent ↔ Users in channel (bidirectional audio)]

### Agent workflows

<Vpd k="NAME" /> supports two distinct workflows for processing conversations:

**Cascading Pipeline (STT-LLM-TTS)**

Chains three specialized AI services in sequence:

1. **STT (Speech-to-Text)**: Converts user's spoken words into text
2. **LLM (Large Language Model)**: Processes the text and generates an intelligent response
3. **TTS (Text-to-Speech)**: Converts the LLM's text response back into natural-sounding speech

This approach gives you full control over each component and flexibility to mix providers. You might use <Vg k="COMPANY" />'s ARES for fast speech recognition, OpenAI's GPT-4o for smart responses, and Azure's neural voices for natural speech output.

**When to use the pipeline:**
- You need to switch providers independently (e.g., different TTS voices by language)
- You want to log and analyze the text transcript
- You're implementing function calling or tool use
- Cost optimization matters (mix free/paid services)
- You need to debug each step separately

[IMAGE: Pipeline workflow showing:
User audio → [STT] → text → [LLM] → text → [TTS] → Agent audio
Three separate boxes with arrows between them
Example latency: 200ms + 300ms + 150ms = 650ms total]

**Multimodal LLM (Voice-to-Voice)**

Uses a single AI model (like OpenAI Realtime API or Google Gemini Live) that processes voice input directly and generates voice output—no separate STT or TTS needed.

These models understand audio natively. They can pick up on tone, emotion, and speech patterns that get lost when converting to text first. This creates faster, more natural conversations with better context understanding.

**When to use MLLM:**
- You want the fastest possible response times
- Natural conversation flow is critical
- Understanding tone and emotion matters
- You prefer simpler configuration (one model vs three)
- You're building a virtual companion or social agent

[IMAGE: MLLM workflow showing:
User audio → [Unified MLLM] → Agent audio
Single box showing voice-in to voice-out
Example latency: 400ms total]

**Note**: When you enable MLLM mode (`enable_mllm: true`), separate ASR, LLM, and TTS configurations are ignored—the MLLM handles everything.

## What can you build?

<Vpd k="NAME" /> enables a wide range of conversational AI applications:

**AI customer support**: Deploy voice agents that handle customer inquiries through web, mobile, or phone (PSTN). Route complex issues to human agents with full conversation context.

**Virtual assistants with avatars**: Combine conversational AI with visual avatars from third-party providers for engaging multimodal experiences in apps, kiosks, or virtual environments.

**Healthcare telehealth**: Build HIPAA-compliant AI triage agents that assess symptoms, schedule appointments, and provide health information before connecting patients with providers.

**Educational tutors**: Create AI tutors that explain concepts, answer questions, and adapt to student learning pace across subjects and languages.

**Interactive voice response (IVR)**: Replace traditional phone menus with natural language interfaces. Users speak naturally instead of navigating numbered options.

**Smart home devices**: Integrate voice AI into IoT devices for natural language control of home automation, information lookup, and interactive assistance.

The engine provides the real-time foundation—low latency, scalable performance, and flexible tools—needed to run production-ready AI experiences across any platform.

## How can you learn more?

This documentation site is organized to guide you from concepts to implementation to production:

**Next steps:**
- **[Agent quickstart](/conversational-ai/get-started/quickstart)**: Build your first backend server that manages agents
- **[Product overview](/conversational-ai/overview/product-overview)**: Explore detailed feature descriptions and capabilities

**Advanced features:**
- **[Custom LLM integration](/conversational-ai/develop/custom-llm)**: Add RAG for knowledge retrieval, function calling, and multi-agent flows
- **[Interrupt agent](/conversational-ai/develop/interrupt-agent)**: Configure how users can interrupt the agent
- **[Webhooks](/conversational-ai/develop/webhooks)**: Monitor agent events and performance in real-time

**Production preparation:**
- **[Pricing](/conversational-ai/overview/pricing)**: Understand costs and optimize your implementation
- **[REST API reference](/conversational-ai/rest-api/agent/join)**: Complete API documentation for agent management

**AI model selection:**
- **[ASR providers](/conversational-ai/models/asr/overview)**: Speech recognition options
- **[LLM providers](/conversational-ai/models/llm/overview)**: Language model options
- **[TTS providers](/conversational-ai/models/tts/overview)**: Text-to-speech options
- **[MLLM providers](/conversational-ai/models/mllm/overview)**: Multimodal voice-to-voice options

Start with the quickstart guide to get hands-on experience, then explore advanced features as your needs evolve.


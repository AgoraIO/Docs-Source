
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import AndroidImplementation from '@docs/shared/conversational-ai/subtitles/implementation/android.mdx';
import SwiftImplementation from '@docs/shared/conversational-ai/subtitles/implementation/swift.mdx';
import WebImplementation from '@docs/shared/conversational-ai/subtitles/implementation/web.mdx';
import AndroidReference from '@docs/shared/conversational-ai/subtitles/reference/android.mdx';
import SwiftReference from '@docs/shared/conversational-ai/subtitles/reference/swift.mdx';
import WebReference from '@docs/shared/conversational-ai/subtitles/reference/web.mdx';

When interacting with conversational AI in real time, you can enable real-time subtitles to display the conversation content. This page explains how to implement real-time subtitles in your app.

## Understand the tech

Agora provides a flexible, scalable, and standardized conversational AI engine toolkit. The toolkit supports **iOS**, **Android**, and **Web** platforms, and encapsulates scenario-based APIs. You can use these APIs to integrate the capabilities of the Agora Chat SDK and Video SDK to enable the following features:

* [`Interrupt agents`](/conversational-ai/develop/interrupt-agent)
* [`Display live subtitles`](/conversational-ai/develop/subtitles)
* [`Receive event notifications`](/conversational-ai/develop/event-notifications)
* [`Set optimal audio parameters`](/conversational-ai/develop/audio-output) (iOS and Android only)

The toolkit retrieves transcription content through the `onTranscriptionUpdated` callback. It supports monitoring different types of subtitle transcription data, including:

* **Agent captions**: Transcriptions of the agent’s speech, including real-time updates and final results.
* **User subtitles**: Transcriptions of the user’s speech, with support for real-time display and status tracking.
* **Transcription status**: Indicates whether transcription is *in progress*, *completed*, or *interrupted*.

The following diagram shows the workflow for implementing live subtitles:

<details>
<summary>Subtitles module workflow</summary>

![](/images/conversational-ai/subtitles-module.svg)
</details>

## Prerequisites

Before you begin, make sure you have implemented the [Quickstart](#) in your project.

## Implementation

This section describes how to receive subtitle content from the subtitle processing module and display it on your app UI.

<Tabs groupId="platform">
<TabItem value="android" label="Android" default>
<AndroidImplementation />
</TabItem>

<TabItem value="swift" label="iOS/macOS" default>
<SwiftImplementation />
</TabItem>

<TabItem value="web" label="Web" default>
<WebImplementation />
</TabItem>
</Tabs>




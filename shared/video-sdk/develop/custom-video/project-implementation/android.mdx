<PlatformWrapper platform="android">

### Custom video capture

The following figure shows the workflow you implement to capture and stream a custom video source in your <Vpl k="CLIENT"/>.

![API call sequence](/images/video-sdk/custom-video-capture.svg)

Take the following steps to implement this workflow:

1. Create a custom video track

    To create a custom video track and obtain the video track ID, call `createCustomVideoTrack` after initializing an instance of `RtcEngine`. To create multiple custom video tracks, call the method multiple times.

    ```java
    int videoTrackId = RtcEngine.createCustomVideoTrack();
    ```

2. Join a channel and publish the custom video track

    Call `joinChannel` to join a channel or `joinChannelEx` to join multiple channels. In the `ChannelMediaOptions` for each channel, set the `customVideoTrackId` to the video track ID you obtained. Set `publishCustomVideoTrack` to `true` to publish the specified custom video track in multiple channels.
    
    1. Join the main channel

        ```java
        ChannelMediaOptions option = new ChannelMediaOptions();
        option.clientRoleType = Constants.CLIENT_ROLE_BROADCASTER;
        option.autoSubscribeAudio = true;
        option.autoSubscribeVideo = true;
        // Publish self-captured video stream
        option.publishCustomVideoTrack = true;
        // Set custom video track ID
        option.customVideoTrackId = videoTrackId;
        // Join the main channel
        int res = engine.joinChannel(accessToken, channelId, 0, option);
        ```

    1. To publish custom video in multiple channels, refer to the following code:

        ```java
        // Set ChannelMediaOptions multiple times and call joinChannelEx multiple times
        ChannelMediaOptions option = a ChannelMediaOptions();
        option.clientRoleType = Constants.CLIENT_ROLE_BROADCASTER;
        option.autoSubscribeAudio = true;
        option.autoSubscribeVideo = true;
        // Publish self-captured video stream
        option.publishCustomVideoTrack = true;
        // Set custom video track ID
        option.customVideoTrackId = videoTrackId;
        // Join multiple channels
        int res = engine.joinChannelEx(accessToken, connection, option, new IRtcEngineEventHandler() {});
        ```

3. Implement your self-capture module

    <Vg k= "COMPANY"/> provides the [VideoFileReader](https://github.com/AgoraIO/API-Examples/blob/main/Android/APIExample/app/src/main/java/io/agora/api/example/utils/VideoFileReader.java) demo project that shows you how to read `YUV` format video data from a local file. In a production environment, create a custom video module for your device using <Vg k= "VSDK"/> based on your business requirements.

4. Push video data to the SDK 

    Before sending captured video frames to <Vg k= "VSDK"/>, integrate your video module with the `VideoFrame`. To ensure audio-video synchronization, best practice is to obtain the current monotonic time from <Vg k= "VSDK"/> and pass it as the timestamp parameter in the `VideoFrame`.

    <Admonition type="info" title="Information">
    To ensure audio-video synchronization, set the timestamp parameter of `VideoFrame` to the system's Monotonic Time. Use `getCurrentMonotonicTimeInMs` to obtain the current Monotonic Time.
    </Admonition>

    Call `pushExternalVideoFrameById` [2/2] to push the captured video frames through the video track to <Vg k= "VSDK"/>. Ensure that the `videoTrackId` matches the track ID you specified when joining the channel. Customize parameters like pixel format, data type, and timestamp in the `VideoFrame`.

    The following code samples demonstrate pushing `I420`, `NV21`, `NV12`, and `Texture` format video data:
    
    <details>
        <summary>I420</summary>
    
        ```java
        private void pushVideoFrameByI420(int trackId, byte[] yuv, int width, int height) {
            // Create an i420Buffer object and store the original YUV data in the buffer
            JavaI420Buffer i420Buffer = JavaI420Buffer.allocate(width, height);
            i420Buffer.getDataY().put(yuv, 0, i420Buffer.getDataY().limit());
            i420Buffer.getDataU().put(yuv, i420Buffer.getDataY().limit(), i420Buffer.getDataU().limit());
            i420Buffer.getDataV().put(yuv, i420Buffer.getDataY().limit() + i420Buffer.getDataU().limit(), i420Buffer.getDataV().limit());
            // Get the current monotonic time from the SDK
            long currentMonotonicTimeInMs = engine.getCurrentMonotonicTimeInMs();
            // Create a VideoFrame object, passing the I420 video frame to be pushed and the monotonic time of the video frame (in nanoseconds)
            VideoFrame videoFrame = new VideoFrame(i420Buffer, 0, currentMonotonicTimeInMs * 1000000);

            // Push the video frame to the SDK through the video track
            int ret = engine.pushExternalVideoFrameById(videoFrame, trackId);
            // Release the memory resources occupied by the i420Buffer object
            i420Buffer.release();

            if (ret != Constants.ERR_OK) {
                Log.w(TAG, "pushExternalVideoFrame error");
            }
        }
        ```
    </details>
    
    <details>
        <summary>NV21</summary>

        ```java
        private void pushVideoFrameByNV21(int trackId, byte[] nv21, int width, height) {
            // Create a frameBuffer object and store the original YUV data in the NV21 format buffer
            VideoFrame.Buffer frameBuffer = new NV21Buffer(nv21, width, height, null);

            // Get the current monotonic time from the SDK
            long currentMonotonicTimeInMs = engine.getCurrentMonotonicTimeInMs();
            // Create a VideoFrame object, pass the NV21 video frame to be pushed and the monotonic time of the video frame (in nanoseconds)
            VideoFrame videoFrame = new VideoFrame(frameBuffer, 0, currentMonotonicTimeInMs * 1000000);

            // Push the video frame to the SDK through the video track
            int ret = engine.pushExternalVideoFrameById(videoFrame, trackId);

            if (ret != Constants.ERR_OK) {
                Log.w(TAG, "pushExternalVideoFrame error");
            }
        }
        ```
    </details>
    <details>
        <summary>NV12</summary>
    
        ```java
        private void pushVideoFrameByNV12(int trackId, ByteBuffer nv12, int width, int height) {
            // Create a frameBuffer object and store the original YUV data in the NV12 format buffer
            VideoFrame.Buffer frameBuffer = new NV12Buffer(width, height, width, height, nv12, null);

            // Get the current monotonic time from the SDK
            long currentMonotonicTimeInMs = engine.getCurrentMonotonicTimeInMs();
            // Create a VideoFrame object, pass the NV12 video frame to be pushed and the monotonic time of the video frame (in nanoseconds)
            VideoFrame videoFrame = new VideoFrame(frameBuffer, 0, currentMonotonicTimeInMs * 1000000);

            // Push the video frame to the SDK through the video track
            int ret = engine.pushExternalVideoFrameById(videoFrame, trackId);

            if (ret != Constants.ERR_OK) {
                Log.w(TAG, "pushExternalVideoFrame error");
            }
        }
        ```
    </details>
        <details>
        <summary>Texture</summary>
    
        ```java
        private void pushVideoFrameByTexture(int trackId, int textureId, VideoFrame.TextureBuffer.Type textureType, int width, int height) {
                // Create a frameBuffer object to store the texture format video frame
                VideoFrame.Buffer frameBuffer = new TextureBuffer(
                    EglBaseProvider.getCurrentEglContext(),
                    width,
                    height,
                    textureType,
                    textureId,
                    new Matrix(),
                    null,
                    null,
                    null
                );
            // Get the current monotonic time from the SDK
            long currentMonotonicTimeInMs = engine.getCurrentMonotonicTimeInMs();
            // Create a VideoFrame object, passing the texture video frame to be pushed and the monotonic time of the video frame (in nanoseconds)
            VideoFrame videoFrame = new VideoFrame(frameBuffer, 0, currentMonotonicTimeInMs * 1000000);

            // Push the video frame to the SDK through the video track
            int ret = engine.pushExternalVideoFrameById(videoFrame, trackId);

            if (ret != Constants.ERR_OK) {
                Log.w(TAG, "pushExternalVideoFrame error");
            }
        }
        ```

        <Admonition type="info" title="Information">
        If the captured custom video format is Texture and remote users experience flickering or distortion in the locally captured video, it is recommended to first duplicate the video data and then send both the original and duplicated video data back to the <Vg k="VSDK"/>. This helps eliminate anomalies during internal data encoding processes.
        </Admonition>
    </details>

    
5. Destroy custom video tracks

    To stop custom video capture and destroy the video track, call `destroyCustomVideoTrack`. To destroy multiple video tracks, call the method for each track.

        ```java
        // Destroy custom video track
        engine.destroyCustomVideoTrack(videoTrack);
        // Leave the channel
        engine.leaveChannelEx(connection);
        ```
### Custom video rendering

To implement custom video rendering in your <Vpl k="CLIENT" />, refer to the following steps:

1. Set up `onCaptureVideoFrame` or `onRenderVideoFrame` callback to obtain the video data to be played.
1. Implement video rendering and playback yourself.

</PlatformWrapper>